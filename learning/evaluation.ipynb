{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import wandb\n",
    "import jax\n",
    "from datetime import datetime\n",
    "from mujoco_playground import registry, wrapper\n",
    "from helper import parse_cfg\n",
    "from omegaconf import OmegaConf\n",
    "from flax.training import checkpoints\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.sac import networks as sac_networks\n",
    "import mediapy as media\n",
    "import hydra\n",
    "from mujoco_playground import registry\n",
    "import functools\n",
    "from mujoco_playground.config import dm_control_suite_params\n",
    "from brax.training.acme import running_statistics\n",
    "from mujoco_playground._src.wrapper import BraxDomainRandomizationVmapWrapper\n",
    "from etils import epath\n",
    "from omegaconf import OmegaConf\n",
    "from mujoco_playground._src import mjx_env\n",
    "from typing import Any, Callable, Dict, Optional, Type, Union, Tuple\n",
    "from mujoco import mjx\n",
    "import numpy\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mujoco_playground._src.wrapper import Wrapper\n",
    "class BraxDomainRandomizationWrapper(Wrapper):\n",
    "  \"\"\"Brax wrapper for domain randomization.\"\"\"\n",
    "  def __init__(\n",
    "      self,\n",
    "      env: mjx_env.MjxEnv,\n",
    "      randomization_fn: Callable[[mjx.Model], Tuple[mjx.Model, mjx.Model]],\n",
    "  ):\n",
    "    super().__init__(env)\n",
    "    self._mjx_model, self._in_axes = randomization_fn(self.env.mjx_model)\n",
    "    self.env.unwrapped._mjx_model = self._mjx_model\n",
    "\n",
    "  # def _env_fn(self, mjx_model: mjx.Model) -> mjx_env.MjxEnv:\n",
    "  #   env = self.env\n",
    "  #   env.unwrapped._mjx_model = mjx_model\n",
    "  #   return env\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> mjx_env.State:\n",
    "    # def reset(mjx_model, rng):\n",
    "    #   env = self._env_fn(mjx_model=mjx_model)\n",
    "    #   return env.reset(rng)\n",
    "\n",
    "    state = self.env.reset(rng)\n",
    "    return state\n",
    "\n",
    "  def step(self, state: mjx_env.State, action: jax.Array) -> mjx_env.State:\n",
    "    # def step(mjx_model, s, a):\n",
    "    #   env = self._env_fn(mjx_model=mjx_model)\n",
    "    #   return env.step(s, a)\n",
    "\n",
    "    res = self.env.step(state, action)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERAS = {\n",
    "    \"AcrobotSwingup\": \"fixed\",\n",
    "    \"AcrobotSwingupSparse\": \"fixed\",\n",
    "    \"BallInCup\": \"cam0\",\n",
    "    \"CartpoleBalance\": \"fixed\",\n",
    "    \"CartpoleBalanceSparse\": \"fixed\",\n",
    "    \"CartpoleSwingup\": \"fixed\",\n",
    "    \"CartpoleSwingupSparse\": \"fixed\",\n",
    "    \"CheetahRun\": \"side\",\n",
    "    \"FingerSpin\": \"cam0\",\n",
    "    \"FingerTurnEasy\": \"cam0\",\n",
    "    \"FingerTurnHard\": \"cam0\",\n",
    "    \"FishSwim\": \"fixed_top\",\n",
    "    \"HopperHop\": \"cam0\",\n",
    "    \"HopperStand\": \"cam0\",\n",
    "    \"HumanoidStand\": \"side\",\n",
    "    \"HumanoidWalk\": \"side\",\n",
    "    \"HumanoidRun\": \"side\",\n",
    "    \"PendulumSwingup\": \"fixed\",\n",
    "    \"PointMass\": \"cam0\",\n",
    "    \"ReacherEasy\": \"fixed\",\n",
    "    \"ReacherHard\": \"fixed\",\n",
    "    \"SwimmerSwimmer6\": \"tracking1\",\n",
    "    \"WalkerRun\": \"side\",\n",
    "    \"WalkerWalk\": \"side\",\n",
    "    \"WalkerStand\": \"side\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from learning.configs.training_config import brax_rambo_config\n",
    "from agents.rambo import networks as rambo_networks\n",
    "from agents.rambo import train as rambo\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def evaluate(cfg):\n",
    "\n",
    "    # Load environment\n",
    "    env = registry.load(cfg.task)\n",
    "    env_cfg = registry.get_default_config(cfg.task)\n",
    "    print(\"env nbody\", env._mj_model.nbody)\n",
    "    print(\"env nv\", env._mj_model.nv)\n",
    "    obs_size = env.observation_size\n",
    "    act_size = env.action_size\n",
    "    rng = jax.random.PRNGKey(cfg.seed)\n",
    "    \n",
    "    if cfg.shift_dynamics:\n",
    "        path = epath.Path(\".\").resolve()\n",
    "        if cfg.shift_dynamics_type == \"deterministic\":\n",
    "            randomization_fn = registry.get_domain_randomizer_eval(cfg.task)\n",
    "            rng, dynamics_rng = jax.random.split(rng)\n",
    "            \n",
    "            randomization_fn = functools.partial(randomization_fn, rng=dynamics_rng, params=env.dr_range)\n",
    "            env = BraxDomainRandomizationWrapper(\n",
    "                env,\n",
    "                randomization_fn=randomization_fn,\n",
    "            )\n",
    "        elif cfg.shift_dynamics_type == \"stochastic\":\n",
    "            rng, dynamics_rng = jax.random.split(rng)\n",
    "            randomization_fn = functools.partial(randomization_fn, rng = dynamics_rng, params=env.dr_range)\n",
    "            env = BraxDomainRandomizationWrapper(\n",
    "                env,\n",
    "                randomization_fn=randomization_fn,\n",
    "            )\n",
    "    if cfg.policy == \"sac\":\n",
    "        sac_params = dm_control_suite_params.brax_sac_config(cfg.task)\n",
    "        sac_training_params = dict(sac_params)\n",
    "        network_factory = sac_networks.make_sac_networks\n",
    "        if \"network_factory\" in sac_params:\n",
    "            del sac_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                sac_networks.make_sac_networks,\n",
    "                **sac_params.network_factory\n",
    "            )\n",
    "        sac_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if sac_params.normalize_observations else None, \n",
    "        )\n",
    "        make_policy_fn = sac_networks.make_inference_fn(sac_network)\n",
    "    elif cfg.policy == \"ppo\":\n",
    "        ppo_params = dm_control_suite_params.brax_ppo_config(cfg.task)\n",
    "        network_factory = ppo_networks.make_ppo_networks\n",
    "        if \"network_factory\" in ppo_params:\n",
    "            network_factory = functools.partial(\n",
    "                ppo_networks.make_ppo_networks,\n",
    "                **ppo_params.network_factory\n",
    "            )\n",
    "        ppo_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if ppo_params.normalize_observations else None,\n",
    "        )\n",
    "\n",
    "        make_policy_fn = ppo_networks.make_inference_fn(ppo_network)\n",
    "    elif cfg.policy == \"rambo\":\n",
    "        rambo_params = brax_rambo_config(cfg.task)\n",
    "        rambo_training_params = dict(rambo_params)\n",
    "        network_factory = rambo_networks.make_rambo_networks\n",
    "        if \"network_factory\" in rambo_params:\n",
    "            del rambo_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                rambo_networks.make_rambo_networks,\n",
    "                **rambo_params.network_factory\n",
    "            )\n",
    "        rambo_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if rambo_params.normalize_observations else None,\n",
    "        )\n",
    "        make_policy_fn = rambo_networks.make_inference_fn(rambo_network)\n",
    "\n",
    "\n",
    "    # Load saved parameters\n",
    "    save_dir = os.path.join(cfg.work_dir, \"models\")\n",
    "    print(f\"Loading parameters from {save_dir}\")\n",
    "    with open(os.path.join(save_dir, f\"{cfg.policy}_params_latest.pkl\"), \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "    jit_inference_fn = jax.jit(make_policy_fn(params,deterministic=True))\n",
    "\n",
    "    jit_reset = jax.jit(env.reset)\n",
    "    jit_step = jax.jit(env.step)\n",
    "\n",
    "    rollout = []\n",
    "    rewards = []\n",
    "    for i in range(1):\n",
    "        # Evaluation loop\n",
    "        state = jit_reset(jax.random.PRNGKey(0))\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        rollout = [state]\n",
    "        rng = jax.random.PRNGKey(cfg.seed)\n",
    "\n",
    "        for _ in range(env_cfg.episode_length):\n",
    "            act_rng, rng = jax.random.split(rng)\n",
    "            action, info = jit_inference_fn(state.obs, act_rng)\n",
    "            state = jit_step(state, action)\n",
    "            rollout.append(state)\n",
    "            total_reward += state.reward\n",
    "        rewards.append(total_reward)\n",
    "    frames = env.render(rollout, camera=CAMERAS[cfg.task])\n",
    "    media.show_video(frames, fps=1.0 / env.dt)\n",
    "    import numpy as np\n",
    "    print(f\"Total reward: {np.array(rewards).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sukchul/distributionally_robust_learning/learning\n",
      "cfg: {'benchmark': 'dm_control', 'task': 'CheetahRun', 'obs': 'state', 'exp_name': 'test', 'checkpoint': '???', 'eval_episodes': 1, 'eval_pi': True, 'eval_value': True, 'eval_freq': 50000, 'policy': 'ppo', 'shift_dynamics': False, 'shift_dynamics_type': 'stochastic', 'eval_randomization': True, 'wandb_project': 'wdsac-exp', 'wandb_entity': 'tjrcjf410-seoul-national-university', 'wandb_silent': False, 'use_wandb': True, 'save_csv': True, 'save_video': True, 'save_agent': True, 'seed': 2, 'work_dir': '???', 'task_title': '???', 'multitask': '???', 'tasks': '???', 'obs_shape': '???', 'action_dim': '???', 'episode_length': '???', 'obs_shapes': '???', 'action_dims': '???', 'episode_lengths': '???', 'seed_steps': '???', 'bin_size': '???', 'real_ratio': '???', 'rollout_length': '???', 'adv_weight': '???', 'batch_size': '???', 'rollout_batch_size': '???', 'n_nominals': '???', 'delta': '???', 'lambda_update_steps': '???', 'single_lambda': '???', 'lmbda_lr': '???', 'init_lmbda': '???', 'distance_type': 'wass', 'custom_wrapper': True}\n",
      "work dir /home/sukchul/distributionally_robust_learning/learning/logs/CheetahRun/2/ppo\n",
      "env nbody 8\n",
      "env nv 9\n",
      "Loading parameters from /home/sukchul/distributionally_robust_learning/learning/logs/CheetahRun/2/ppo/models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"ppo\"\n",
    "cfg.shift_dynamics = False\n",
    "cfg.shift_dynamics_type = \"stochastic\"  # or \"deterministic\"\n",
    "cfg.seed=2\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "# cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "evaluate(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
