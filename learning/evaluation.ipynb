{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "# os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # change to your GPU id\n",
    "os.environ['PYTHONPATH'] = '/home/sukchul/distributionally_robust_learning'\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from helper import parse_cfg\n",
    "from omegaconf import OmegaConf\n",
    "from flax.training import checkpoints\n",
    "\n",
    "# import mediapy as media\n",
    "import hydra\n",
    "import functools\n",
    "from brax.training.acme import running_statistics\n",
    "from etils import epath\n",
    "from omegaconf import OmegaConf\n",
    "from custom_envs import mjx_env\n",
    "from typing import Any, Callable, Dict, Optional, Type, Union, Tuple\n",
    "from mujoco import mjx\n",
    "import numpy\n",
    "from custom_envs import registry, dm_control_suite, locomotion\n",
    "import jax\n",
    "from learning.agents.ppo import networks as ppo_networks\n",
    "from learning.agents.sac import networks as sac_networks\n",
    "from learning.agents.td3 import networks as td3_networks\n",
    "from learning.configs import dm_control_training_config, locomotion_training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.module.wrapper.wrapper import Wrapper\n",
    "class BraxDomainRandomizationWrapper(Wrapper):\n",
    "  \"\"\"Brax wrapper for domain randomization.\"\"\"\n",
    "  def __init__(\n",
    "      self,\n",
    "      env: mjx_env.MjxEnv,\n",
    "      randomization_fn: Callable[[mjx.Model], Tuple[mjx.Model, mjx.Model]],\n",
    "  ):\n",
    "    super().__init__(env)\n",
    "    self._mjx_model, self._in_axes = randomization_fn(self.env.mjx_model)\n",
    "    self.env.unwrapped._mjx_model = self._mjx_model\n",
    "\n",
    "  # def _env_fn(self, mjx_model: mjx.Model) -> mjx_env.MjxEnv:\n",
    "  #   env = self.env\n",
    "  #   env.unwrapped._mjx_model = mjx_model\n",
    "  #   return env\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> mjx_env.State:\n",
    "    # def reset(mjx_model, rng):\n",
    "    #   env = self._env_fn(mjx_model=mjx_model)\n",
    "    #   return env.reset(rng)\n",
    "\n",
    "    state = self.env.reset(rng)\n",
    "    return state\n",
    "\n",
    "  def step(self, state: mjx_env.State, action: jax.Array) -> mjx_env.State:\n",
    "    # def step(mjx_model, s, a):\n",
    "    #   env = self._env_fn(mjx_model=mjx_model)\n",
    "    #   return env.step(s, a)\n",
    "\n",
    "    res = self.env.step(state, action)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERAS = {\n",
    "    \"AcrobotSwingup\": \"fixed\",\n",
    "    \"AcrobotSwingupSparse\": \"fixed\",\n",
    "    \"BallInCup\": \"cam0\",\n",
    "    \"CartpoleBalance\": \"fixed\",\n",
    "    \"CartpoleBalanceSparse\": \"fixed\",\n",
    "    \"CartpoleSwingup\": \"fixed\",\n",
    "    \"CartpoleSwingupSparse\": \"fixed\",\n",
    "    \"CheetahRun\": \"side\",\n",
    "    \"FingerSpin\": \"cam0\",\n",
    "    \"FingerTurnEasy\": \"cam0\",\n",
    "    \"FingerTurnHard\": \"cam0\",\n",
    "    \"FishSwim\": \"fixed_top\",\n",
    "    \"HopperHop\": \"cam0\",\n",
    "    \"HopperStand\": \"cam0\",\n",
    "    \"HumanoidStand\": \"side\",\n",
    "    \"HumanoidWalk\": \"side\",\n",
    "    \"HumanoidRun\": \"side\",\n",
    "    \"PendulumSwingup\": \"fixed\",\n",
    "    \"PointMass\": \"cam0\",\n",
    "    \"ReacherEasy\": \"fixed\",\n",
    "    \"ReacherHard\": \"fixed\",\n",
    "    \"SwimmerSwimmer6\": \"tracking1\",\n",
    "    \"WalkerRun\": \"side\",\n",
    "    \"WalkerWalk\": \"side\",\n",
    "    \"WalkerStand\": \"side\",    \n",
    "    \"Go1Handstand\": \"side\",\n",
    "    \"Go1JoystickRoughTerrain\": \"track\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents.flowsac import networks as flowsac_networks\n",
    "from agents.rambo import networks as rambo_networks\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def evaluate(cfg):\n",
    "\n",
    "    # Load environment\n",
    "    env = registry.load(cfg.task)\n",
    "    env_cfg = registry.get_default_config(cfg.task)\n",
    "    print(\"env nbody\", env._mj_model.nbody)\n",
    "    print(\"env nv\", env._mj_model.nv)\n",
    "    obs_size = env.observation_size\n",
    "    act_size = env.action_size\n",
    "    rng = jax.random.PRNGKey(cfg.eval_seed)\n",
    "    \n",
    "    if cfg.randomization:\n",
    "        path = epath.Path(\".\").resolve()\n",
    "\n",
    "        randomization_fn = registry.get_domain_randomizer_eval(cfg.task)\n",
    "        rng, dynamics_rng = jax.random.split(rng)\n",
    "        randomization_fn = functools.partial(randomization_fn, rng = dynamics_rng, params=env.dr_range)\n",
    "        env = BraxDomainRandomizationWrapper(\n",
    "            env,\n",
    "            randomization_fn=randomization_fn,\n",
    "        )\n",
    "    if cfg.policy == \"sac\":\n",
    "        if cfg.task in dm_control_suite._envs:\n",
    "            sac_params = dm_control_training_config.brax_sac_config(cfg.task)\n",
    "        elif cfg.task in locomotion._envs:\n",
    "            sac_params = locomotion_training_config.locomotion_sac_config(cfg.task)\n",
    "        sac_training_params = dict(sac_params)\n",
    "        network_factory = sac_networks.make_sac_networks\n",
    "        if \"network_factory\" in sac_params:\n",
    "            del sac_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                sac_networks.make_sac_networks,\n",
    "                **sac_params.network_factory\n",
    "            )\n",
    "        sac_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if sac_params.normalize_observations else None, \n",
    "        )\n",
    "        make_policy_fn = sac_networks.make_inference_fn(sac_network)\n",
    "    elif cfg.policy == \"ppo\":\n",
    "        if cfg.task in dm_control_suite._envs:\n",
    "            ppo_params = dm_control_training_config.brax_ppo_config(cfg.task)\n",
    "        elif cfg.task in locomotion._envs:\n",
    "            ppo_params = locomotion_training_config.locomotion_ppo_config(cfg.task)\n",
    "        network_factory = ppo_networks.make_ppo_networks\n",
    "        if \"network_factory\" in ppo_params:\n",
    "            network_factory = functools.partial(\n",
    "                ppo_networks.make_ppo_networks,\n",
    "                **ppo_params.network_factory\n",
    "            )\n",
    "        ppo_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if ppo_params.normalize_observations else None,\n",
    "        )\n",
    "\n",
    "        make_policy_fn = ppo_networks.make_inference_fn(ppo_network)\n",
    "    elif cfg.policy == \"rambo\":\n",
    "        rambo_params = brax_rambo_config(cfg.task)\n",
    "        rambo_training_params = dict(rambo_params)\n",
    "        network_factory = rambo_networks.make_rambo_networks\n",
    "        if \"network_factory\" in rambo_params:\n",
    "            del rambo_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                rambo_networks.make_rambo_networks,\n",
    "                **rambo_params.network_factory\n",
    "            )\n",
    "        rambo_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if rambo_params.normalize_observations else None,\n",
    "        )\n",
    "        make_policy_fn = rambo_networks.make_inference_fn(rambo_network)\n",
    "    elif cfg.policy == \"flowsac\":\n",
    "        flowsac_params = brax_flowsac_config(cfg.task)\n",
    "        flowsac_training_params = dict(flowsac_params)\n",
    "        network_factory = flowsac_networks.make_flowsac_networks\n",
    "        if \"network_factory\" in flowsac_params:\n",
    "            del flowsac_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                flowsac_networks.make_flowsac_networks,\n",
    "                **flowsac_params.network_factory\n",
    "            )\n",
    "        flowsac_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if flowsac_params.normalize_observations else None,\n",
    "            dynamics_param_size=len(env.dr_range)\n",
    "        )\n",
    "        make_policy_fn = flowsac_networks.make_inference_fn(flowsac_network)\n",
    "    elif cfg.policy == \"td3\":\n",
    "        if cfg.task in dm_control_suite._envs:\n",
    "            td3_params = dm_control_training_config.brax_td3_config(cfg.task)\n",
    "        elif cfg.task in locomotion._envs:\n",
    "            td3_params= locomotion_training_config.locomotion_td3_config(cfg.task)\n",
    "        td3_training_params = dict(td3_params)\n",
    "        network_factory = td3_networks.make_td3_networks\n",
    "        if \"network_factory\" in td3_params:\n",
    "            del td3_training_params[\"network_factory\"]\n",
    "            network_factory = functools.partial(\n",
    "                td3_networks.make_td3_networks,\n",
    "                **td3_params.network_factory\n",
    "            )\n",
    "        td3_network = network_factory(\n",
    "            observation_size=obs_size,\n",
    "            action_size=act_size,\n",
    "            preprocess_observations_fn=running_statistics.normalize if td3_params.normalize_observations else None, \n",
    "        )\n",
    "        make_policy_fn = td3_networks.make_inference_fn(td3_network)\n",
    "    # Load saved parameters\n",
    "    save_dir = os.path.join(cfg.work_dir, \"models\")\n",
    "    print(f\"Loading parameters from {save_dir}\")\n",
    "    with open(os.path.join(save_dir, f\"{cfg.policy}_params_latest.pkl\"), \"rb\") as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "    jit_inference_fn = jax.jit(make_policy_fn(params,deterministic=True))\n",
    "\n",
    "    jit_reset = jax.jit(env.reset)\n",
    "    jit_step = jax.jit(env.step)\n",
    "\n",
    "    rollout = []\n",
    "    rewards = []\n",
    "    rng =jax.random.PRNGKey(cfg.eval_seed)\n",
    "    reset_rng, rng = jax.random.split(rng)\n",
    "    reset_rngs = jax.random.split(reset_rng, 10)\n",
    "    for i in range(10):\n",
    "        # Evaluation loop\n",
    "        \n",
    "        state = jit_reset(reset_rngs[i])\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        rollout = [state]\n",
    "\n",
    "        for _ in range(env_cfg.episode_length):\n",
    "            act_rng, rng = jax.random.split(rng)\n",
    "            action, info = jit_inference_fn(state.obs, act_rng)\n",
    "            state = jit_step(state, action)\n",
    "            rollout.append(state)\n",
    "            total_reward += state.reward\n",
    "        rewards.append(total_reward)\n",
    "    frames = env.render(rollout, camera=CAMERAS[cfg.task])\n",
    "    media.show_video(frames, fps=1.0 / env.dt)\n",
    "    import numpy as np\n",
    "    print(f\"Total reward: {np.array(rewards).mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/users/tjrcjf410/distributionally_robust_learning/learning\n",
      "cfg: {'benchmark': 'dm_control', 'task': 'CheetahRun', 'obs': 'state', 'exp_name': 'test', 'checkpoint': '???', 'eval_episodes': 1, 'eval_pi': True, 'eval_value': True, 'eval_freq': 50000, 'impl': 'warp', 'eval_with_training_env': False, 'policy': 'sac', 'asymmetric_critic': True, 'distributional_q': False, 'randomization': False, 'eval_randomization': True, 'wandb_project': 'test-algorithms', 'wandb_entity': 'tjrcjf410-seoul-national-university', 'wandb_silent': False, 'use_wandb': True, 'save_csv': True, 'comment': '', 'save_video': True, 'save_agent': True, 'seed': 0, 'work_dir': '???', 'task_title': '???', 'multitask': '???', 'tasks': '???', 'obs_shape': '???', 'action_dim': '???', 'episode_length': '???', 'obs_shapes': '???', 'action_dims': '???', 'episode_lengths': '???', 'seed_steps': '???', 'bin_size': '???', 'real_ratio': '???', 'rollout_length': '???', 'adv_weight': '???', 'batch_size': '???', 'rollout_batch_size': '???', 'n_nominals': '???', 'delta': '???', 'lambda_update_steps': '???', 'single_lambda': False, 'lmbda_lr': '???', 'init_lmbda': '???', 'distance_type': 'wass', 'custom_wrapper': True, 'adv_wrapper': True, 'flow_lr': '???', 'dr_train_ratio': 1.0, 'simba': False, 'horizon': 3, 'omega_distance_threshold': 0.1}\n",
      "work dir /raid/users/tjrcjf410/distributionally_robust_learning/learning/logs/CheetahRun/0/sac\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg_path= epath.Path(\".\").resolve()\n",
    "print(cfg_path)\n",
    "cfg_path = os.path.join(cfg_path, \"config.yaml\")\n",
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "cfg.task=\"CheetahRun\"\n",
    "cfg.policy=\"sac\"\n",
    "cfg.seed=0\n",
    "cfg.randomization=False\n",
    "\n",
    "print(\"cfg:\", cfg)\n",
    "cfg = parse_cfg(cfg)\n",
    "cfg.eval_seed= 55\n",
    "print(\"work dir\", cfg.work_dir)\n",
    "\n",
    "# cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "#843.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = registry.load(cfg.task)\n",
    "env._config\n",
    "env.mjx_model.nbody\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomization_fn = registry.get_domain_randomizer_eval(cfg.task)\n",
    "rng = jax.random.PRNGKey(cfg.seed)\n",
    "rng, dynamics_rng = jax.random.split(rng)\n",
    "env = registry.load(cfg.task)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
