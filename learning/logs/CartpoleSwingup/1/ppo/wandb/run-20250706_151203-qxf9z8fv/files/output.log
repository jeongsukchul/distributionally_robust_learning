INFO:2025-07-06 15:12:04,660:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 15:12:04,660][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 15:12:04,876][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:12:04,877][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:12:05,315][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 15:12:06,118][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:12:06,119][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:12:15,535][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:12:15,535][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:12:29,052][absl][INFO] - {'eval/walltime': 13.423279285430908, 'eval/episode_reward': Array(34.508, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(826.759, dtype=float32), 'eval/episode_reward/small_control': Array(941.28, dtype=float32), 'eval/episode_reward/small_velocity': Array(911.499, dtype=float32), 'eval/episode_reward/upright': Array(55.745, dtype=float32), 'eval/episode_reward_std': Array(25.491, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(65.51, dtype=float32), 'eval/episode_reward/small_control_std': Array(2.722, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(47.326, dtype=float32), 'eval/episode_reward/upright_std': Array(49.662, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 13.423279285430908, 'eval/sps': 9535.672861915798}
[2025-07-06 15:12:29,068][absl][INFO] - starting iteration 0 23.75236678123474
[2025-07-06 15:13:20,035][absl][INFO] - {'eval/walltime': 13.66371750831604, 'training/sps': np.float64(136612.27227929095), 'training/walltime': 50.370877265930176, 'training/entropy_loss': Array(-0.006, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(233.249, dtype=float32), 'training/v_loss': Array(233.26, dtype=float32), 'eval/episode_reward': Array(195.326, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(761.754, dtype=float32), 'eval/episode_reward/small_control': Array(925.191, dtype=float32), 'eval/episode_reward/small_velocity': Array(582.582, dtype=float32), 'eval/episode_reward/upright': Array(478.517, dtype=float32), 'eval/episode_reward_std': Array(17.194, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(20.556, dtype=float32), 'eval/episode_reward/small_control_std': Array(8.787, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(23.098, dtype=float32), 'eval/episode_reward/upright_std': Array(19.213, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.24043822288513184, 'eval/sps': 532361.2796005041}
[2025-07-06 15:13:20,050][absl][INFO] - starting iteration 1 74.73460483551025
[2025-07-06 15:13:52,515][absl][INFO] - {'eval/walltime': 13.903383731842041, 'training/sps': np.float64(213628.95068789655), 'training/walltime': 82.58224177360535, 'training/entropy_loss': Array(-0.004, dtype=float32), 'training/policy_loss': Array(-0.008, dtype=float32), 'training/total_loss': Array(288.559, dtype=float32), 'training/v_loss': Array(288.571, dtype=float32), 'eval/episode_reward': Array(763.536, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(925.542, dtype=float32), 'eval/episode_reward/small_control': Array(962.423, dtype=float32), 'eval/episode_reward/small_velocity': Array(947.787, dtype=float32), 'eval/episode_reward/upright': Array(845.362, dtype=float32), 'eval/episode_reward_std': Array(53.227, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(23.518, dtype=float32), 'eval/episode_reward/small_control_std': Array(3.102, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(22.916, dtype=float32), 'eval/episode_reward/upright_std': Array(37.131, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.23966622352600098, 'eval/sps': 534076.0918115502}
[2025-07-06 15:13:52,536][absl][INFO] - starting iteration 2 107.22080993652344
[2025-07-06 15:14:25,005][absl][INFO] - {'eval/walltime': 14.143730401992798, 'training/sps': np.float64(213611.6061614818), 'training/walltime': 114.79622173309326, 'training/entropy_loss': Array(-0.002, dtype=float32), 'training/policy_loss': Array(-0.012, dtype=float32), 'training/total_loss': Array(116.335, dtype=float32), 'training/v_loss': Array(116.35, dtype=float32), 'eval/episode_reward': Array(786.146, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(901.843, dtype=float32), 'eval/episode_reward/small_control': Array(971.373, dtype=float32), 'eval/episode_reward/small_velocity': Array(958.14, dtype=float32), 'eval/episode_reward/upright': Array(883.393, dtype=float32), 'eval/episode_reward_std': Array(52.23, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(25.06, dtype=float32), 'eval/episode_reward/small_control_std': Array(3.541, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(31.957, dtype=float32), 'eval/episode_reward/upright_std': Array(28.852, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.24034667015075684, 'eval/sps': 532564.0663950631}
[2025-07-06 15:14:25,027][absl][INFO] - starting iteration 3 139.71151113510132
[2025-07-06 15:14:57,353][absl][INFO] - {'eval/walltime': 14.38701343536377, 'training/sps': np.float64(214574.37914437486), 'training/walltime': 146.865660905838, 'training/entropy_loss': Array(0.002, dtype=float32), 'training/policy_loss': Array(-0.018, dtype=float32), 'training/total_loss': Array(65.399, dtype=float32), 'training/v_loss': Array(65.416, dtype=float32), 'eval/episode_reward': Array(834.159, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(934.819, dtype=float32), 'eval/episode_reward/small_control': Array(982.813, dtype=float32), 'eval/episode_reward/small_velocity': Array(960.286, dtype=float32), 'eval/episode_reward/upright': Array(886.963, dtype=float32), 'eval/episode_reward_std': Array(23.203, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(22.576, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.135, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(1.03, dtype=float32), 'eval/episode_reward/upright_std': Array(0.849, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.24328303337097168, 'eval/sps': 526136.1560089494}
[2025-07-06 15:14:57,367][absl][INFO] - starting iteration 4 172.05125546455383
[2025-07-06 15:15:29,821][absl][INFO] - {'eval/walltime': 14.625202178955078, 'training/sps': np.float64(213679.17873851108), 'training/walltime': 179.06945371627808, 'training/entropy_loss': Array(0.003, dtype=float32), 'training/policy_loss': Array(-0.006, dtype=float32), 'training/total_loss': Array(131.843, dtype=float32), 'training/v_loss': Array(131.846, dtype=float32), 'eval/episode_reward': Array(829.803, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(936.671, dtype=float32), 'eval/episode_reward/small_control': Array(983.532, dtype=float32), 'eval/episode_reward/small_velocity': Array(957.828, dtype=float32), 'eval/episode_reward/upright': Array(880.235, dtype=float32), 'eval/episode_reward_std': Array(39.209, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(20.429, dtype=float32), 'eval/episode_reward/small_control_std': Array(4.294, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(15.567, dtype=float32), 'eval/episode_reward/upright_std': Array(18.616, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.2381887435913086, 'eval/sps': 537388.9549525743}
[2025-07-06 15:15:29,830][absl][INFO] - starting iteration 5 204.51504945755005
[2025-07-06 15:16:02,597][absl][INFO] - {'eval/walltime': 14.89112639427185, 'training/sps': np.float64(211818.3512082359), 'training/walltime': 211.55615735054016, 'training/entropy_loss': Array(0.005, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(46.717, dtype=float32), 'training/v_loss': Array(46.717, dtype=float32), 'eval/episode_reward': Array(846.117, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(947.907, dtype=float32), 'eval/episode_reward/small_control': Array(983.466, dtype=float32), 'eval/episode_reward/small_velocity': Array(956.872, dtype=float32), 'eval/episode_reward/upright': Array(885.19, dtype=float32), 'eval/episode_reward_std': Array(16.577, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(12.291, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.986, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(4.168, dtype=float32), 'eval/episode_reward/upright_std': Array(4.937, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.26592421531677246, 'eval/sps': 481340.1436477859}
[2025-07-06 15:16:02,606][absl][INFO] - starting iteration 6 237.29093408584595
[2025-07-06 15:16:36,364][absl][INFO] - {'eval/walltime': 15.168967485427856, 'training/sps': np.float64(205632.2056433469), 'training/walltime': 245.02017617225647, 'training/entropy_loss': Array(0.006, dtype=float32), 'training/policy_loss': Array(-0.005, dtype=float32), 'training/total_loss': Array(27.608, dtype=float32), 'training/v_loss': Array(27.607, dtype=float32), 'eval/episode_reward': Array(850.262, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(948.494, dtype=float32), 'eval/episode_reward/small_control': Array(985.011, dtype=float32), 'eval/episode_reward/small_velocity': Array(957.415, dtype=float32), 'eval/episode_reward/upright': Array(887.428, dtype=float32), 'eval/episode_reward_std': Array(10.338, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(9.247, dtype=float32), 'eval/episode_reward/small_control_std': Array(0.675, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(0.908, dtype=float32), 'eval/episode_reward/upright_std': Array(1.126, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.27784109115600586, 'eval/sps': 460694.9946368044}
[2025-07-06 15:16:36,391][absl][INFO] - starting iteration 7 271.0758924484253
[2025-07-06 15:17:09,787][absl][INFO] - {'eval/walltime': 15.444021463394165, 'training/sps': np.float64(207860.60665477038), 'training/walltime': 278.1254389286041, 'training/entropy_loss': Array(0.008, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(17.358, dtype=float32), 'training/v_loss': Array(17.357, dtype=float32), 'eval/episode_reward': Array(775.603, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(921.744, dtype=float32), 'eval/episode_reward/small_control': Array(981.904, dtype=float32), 'eval/episode_reward/small_velocity': Array(920.867, dtype=float32), 'eval/episode_reward/upright': Array(850.908, dtype=float32), 'eval/episode_reward_std': Array(171.809, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(38.578, dtype=float32), 'eval/episode_reward/small_control_std': Array(13.732, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(108.34, dtype=float32), 'eval/episode_reward/upright_std': Array(103.676, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.2750539779663086, 'eval/sps': 465363.2023299759}
[2025-07-06 15:17:09,818][absl][INFO] - starting iteration 8 304.5021586418152
[2025-07-06 15:17:43,285][absl][INFO] - {'eval/walltime': 15.6842622756958, 'training/sps': np.float64(207186.65317054544), 'training/walltime': 311.3383891582489, 'training/entropy_loss': Array(0.009, dtype=float32), 'training/policy_loss': Array(-0.007, dtype=float32), 'training/total_loss': Array(70.445, dtype=float32), 'training/v_loss': Array(70.444, dtype=float32), 'eval/episode_reward': Array(852.935, dtype=float32), 'eval/episode_reward/angle_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds': Array(0., dtype=float32), 'eval/episode_reward/centered': Array(953.5, dtype=float32), 'eval/episode_reward/small_control': Array(984.7, dtype=float32), 'eval/episode_reward/small_velocity': Array(956.716, dtype=float32), 'eval/episode_reward/upright': Array(886.199, dtype=float32), 'eval/episode_reward_std': Array(3.948, dtype=float32), 'eval/episode_reward/angle_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/cart_in_bounds_std': Array(0., dtype=float32), 'eval/episode_reward/centered_std': Array(3.873, dtype=float32), 'eval/episode_reward/small_control_std': Array(0.702, dtype=float32), 'eval/episode_reward/small_velocity_std': Array(0.881, dtype=float32), 'eval/episode_reward/upright_std': Array(1.018, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 0.24024081230163574, 'eval/sps': 532798.7312966994}
[2025-07-06 15:17:44,120][absl][INFO] - total steps: 61931520
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/ros/mujoco_playground/learning/train.py", line 187, in train
    train_ppo(cfg)
  File "/home/ros/mujoco_playground/learning/train.py", line 135, in train_ppo
    print(f"time to jit: {times[1] - times[0]}")
                          ~~~~~^^^
IndexError: list index out of range

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
