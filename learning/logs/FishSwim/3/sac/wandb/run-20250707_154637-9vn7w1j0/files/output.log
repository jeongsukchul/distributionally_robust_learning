INFO:2025-07-07 15:46:38,853:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 15:46:38,853][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 15:46:39,011][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:39,011][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:39,250][absl][INFO] - local_device_count: 1; total_device_count: 1
[2025-07-07 15:46:39,641][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:39,641][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:45,501][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:45,501][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:59,036][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:59,036][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:47:31,713][absl][INFO] - {'eval/walltime': 32.53706765174866, 'eval/episode_reward': Array(68.493, dtype=float32), 'eval/episode_reward/in_target': Array(4.573, dtype=float32), 'eval/episode_reward/upright': Array(515.933, dtype=float32), 'eval/episode_reward_std': Array(35.917, dtype=float32), 'eval/episode_reward/in_target_std': Array(26.554, dtype=float32), 'eval/episode_reward/upright_std': Array(243.381, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 32.53706765174866, 'eval/sps': 3933.974670674443}
 eval/walltime :  32.53706765174866
 eval/episode_reward :  68.49324035644531
 eval/episode_reward/in_target :  4.573215961456299
 eval/episode_reward/upright :  515.9334106445312
 eval/episode_reward_std :  35.91741943359375
 eval/episode_reward/in_target_std :  26.553640365600586
 eval/episode_reward/upright_std :  243.38084411621094
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  32.53706765174866
 eval/sps :  3933.974670674443
[2025-07-07 15:47:48,728][absl][INFO] - replay size after prefill 8192
[2025-07-07 15:47:48,763][absl][INFO] - step 0
[2025-07-07 15:48:46,429][absl][INFO] - {'eval/walltime': 36.387571573257446, 'training/sps': 10312.454779861704, 'training/walltime': 70.83995842933655, 'training/actor_loss': Array(-33.679, dtype=float32), 'training/alpha': Array(0.118, dtype=float32), 'training/alpha_loss': Array(0.688, dtype=float32), 'training/buffer_current_size': Array(285632., dtype=float32), 'training/critic_loss': Array(0.031, dtype=float32), 'eval/episode_reward': Array(85.885, dtype=float32), 'eval/episode_reward/in_target': Array(21.429, dtype=float32), 'eval/episode_reward/upright': Array(537.078, dtype=float32), 'eval/episode_reward_std': Array(94.976, dtype=float32), 'eval/episode_reward/in_target_std': Array(99.916, dtype=float32), 'eval/episode_reward/upright_std': Array(253.332, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.850503921508789, 'eval/sps': 33242.40219182642}
 eval/walltime :  36.387571573257446
 training/sps :  10312.454779861704
 training/walltime :  70.83995842933655
 training/actor_loss :  -33.67851638793945
 training/alpha :  0.11751604080200195
 training/alpha_loss :  0.6877983808517456
 training/buffer_current_size :  285632.0
 training/critic_loss :  0.030755046755075455
 eval/episode_reward :  85.88520812988281
 eval/episode_reward/in_target :  21.429061889648438
 eval/episode_reward/upright :  537.0782470703125
 eval/episode_reward_std :  94.97627258300781
 eval/episode_reward/in_target_std :  99.9163818359375
 eval/episode_reward/upright_std :  253.33226013183594
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.850503921508789
 eval/sps :  33242.40219182642
[2025-07-07 15:48:46,435][absl][INFO] - step 562944
[2025-07-07 15:49:17,910][absl][INFO] - {'eval/walltime': 40.13134455680847, 'training/sps': 20008.943943456623, 'training/walltime': 98.56515979766846, 'training/actor_loss': Array(-16.599, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(840383.94, dtype=float32), 'training/critic_loss': Array(0.063, dtype=float32), 'eval/episode_reward': Array(120.725, dtype=float32), 'eval/episode_reward/in_target': Array(61.628, dtype=float32), 'eval/episode_reward/upright': Array(534.404, dtype=float32), 'eval/episode_reward_std': Array(165.162, dtype=float32), 'eval/episode_reward/in_target_std': Array(186.418, dtype=float32), 'eval/episode_reward/upright_std': Array(234.645, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.7437729835510254, 'eval/sps': 34190.10729614007}
 eval/walltime :  40.13134455680847
 training/sps :  20008.943943456623
 training/walltime :  98.56515979766846
 training/actor_loss :  -16.59925651550293
 training/alpha :  0.0017171473009511828
 training/alpha_loss :  -2.04383995878743e-05
 training/buffer_current_size :  840383.9375
 training/critic_loss :  0.06258995831012726
 eval/episode_reward :  120.72535705566406
 eval/episode_reward/in_target :  61.628379821777344
 eval/episode_reward/upright :  534.404296875
 eval/episode_reward_std :  165.162109375
 eval/episode_reward/in_target_std :  186.4183349609375
 eval/episode_reward/upright_std :  234.64517211914062
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.7437729835510254
 eval/sps :  34190.10729614007
[2025-07-07 15:49:17,915][absl][INFO] - step 1117696
[2025-07-07 15:49:49,614][absl][INFO] - {'eval/walltime': 44.04436373710632, 'training/sps': 19970.219675284265, 'training/walltime': 126.3441231250763, 'training/actor_loss': Array(-14.889, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(1.395e+06, dtype=float32), 'training/critic_loss': Array(0.05, dtype=float32), 'eval/episode_reward': Array(178.517, dtype=float32), 'eval/episode_reward/in_target': Array(124.249, dtype=float32), 'eval/episode_reward/upright': Array(558.396, dtype=float32), 'eval/episode_reward_std': Array(232.169, dtype=float32), 'eval/episode_reward/in_target_std': Array(262.845, dtype=float32), 'eval/episode_reward/upright_std': Array(231.332, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.9130191802978516, 'eval/sps': 32711.31423134422}
 eval/walltime :  44.04436373710632
 training/sps :  19970.219675284265
 training/walltime :  126.3441231250763
 training/actor_loss :  -14.888824462890625
 training/alpha :  0.001858393894508481
 training/alpha_loss :  -5.572151167143602e-06
 training/buffer_current_size :  1395135.875
 training/critic_loss :  0.05012236163020134
 eval/episode_reward :  178.51693725585938
 eval/episode_reward/in_target :  124.24858856201172
 eval/episode_reward/upright :  558.3955078125
 eval/episode_reward_std :  232.16864013671875
 eval/episode_reward/in_target_std :  262.8445739746094
 eval/episode_reward/upright_std :  231.33164978027344
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.9130191802978516
 eval/sps :  32711.31423134422
[2025-07-07 15:49:49,625][absl][INFO] - step 1672448
[2025-07-07 15:50:21,252][absl][INFO] - {'eval/walltime': 47.948872327804565, 'training/sps': 20015.74501197909, 'training/walltime': 154.05990386009216, 'training/actor_loss': Array(-15.928, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(1.95e+06, dtype=float32), 'training/critic_loss': Array(0.04, dtype=float32), 'eval/episode_reward': Array(187.046, dtype=float32), 'eval/episode_reward/in_target': Array(137.441, dtype=float32), 'eval/episode_reward/upright': Array(534.282, dtype=float32), 'eval/episode_reward_std': Array(252.693, dtype=float32), 'eval/episode_reward/in_target_std': Array(286.332, dtype=float32), 'eval/episode_reward/upright_std': Array(257.344, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.904508590698242, 'eval/sps': 32782.61451516228}
 eval/walltime :  47.948872327804565
 training/sps :  20015.74501197909
 training/walltime :  154.05990386009216
 training/actor_loss :  -15.927877426147461
 training/alpha :  0.0017078046221286058
 training/alpha_loss :  9.462075468036346e-06
 training/buffer_current_size :  1949887.875
 training/critic_loss :  0.04005807638168335
 eval/episode_reward :  187.04595947265625
 eval/episode_reward/in_target :  137.44082641601562
 eval/episode_reward/upright :  534.2821655273438
 eval/episode_reward_std :  252.69337463378906
 eval/episode_reward/in_target_std :  286.3321838378906
 eval/episode_reward/upright_std :  257.34368896484375
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.904508590698242
 eval/sps :  32782.61451516228
[2025-07-07 15:50:21,259][absl][INFO] - step 2227200
[2025-07-07 15:50:53,233][absl][INFO] - {'eval/walltime': 51.798868894577026, 'training/sps': 19729.90141884265, 'training/walltime': 182.17722630500793, 'training/actor_loss': Array(-17.349, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(2.505e+06, dtype=float32), 'training/critic_loss': Array(0.032, dtype=float32), 'eval/episode_reward': Array(238.716, dtype=float32), 'eval/episode_reward/in_target': Array(189.477, dtype=float32), 'eval/episode_reward/upright': Array(583.388, dtype=float32), 'eval/episode_reward_std': Array(280.128, dtype=float32), 'eval/episode_reward/in_target_std': Array(318.363, dtype=float32), 'eval/episode_reward/upright_std': Array(234.323, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.849996566772461, 'eval/sps': 33246.78289448588}
 eval/walltime :  51.798868894577026
 training/sps :  19729.90141884265
 training/walltime :  182.17722630500793
 training/actor_loss :  -17.348920822143555
 training/alpha :  0.0015671896981075406
 training/alpha_loss :  -1.3185865100240335e-05
 training/buffer_current_size :  2504640.0
 training/critic_loss :  0.03180063143372536
 eval/episode_reward :  238.7162322998047
 eval/episode_reward/in_target :  189.47744750976562
 eval/episode_reward/upright :  583.3877563476562
 eval/episode_reward_std :  280.12786865234375
 eval/episode_reward/in_target_std :  318.363037109375
 eval/episode_reward/upright_std :  234.3226318359375
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.849996566772461
 eval/sps :  33246.78289448588
[2025-07-07 15:50:53,239][absl][INFO] - step 2781952
[2025-07-07 15:51:24,941][absl][INFO] - {'eval/walltime': 55.661505460739136, 'training/sps': 19930.9089876584, 'training/walltime': 210.0109794139862, 'training/actor_loss': Array(-19.055, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(3.059e+06, dtype=float32), 'training/critic_loss': Array(0.029, dtype=float32), 'eval/episode_reward': Array(273.595, dtype=float32), 'eval/episode_reward/in_target': Array(232.899, dtype=float32), 'eval/episode_reward/upright': Array(558.47, dtype=float32), 'eval/episode_reward_std': Array(291.44, dtype=float32), 'eval/episode_reward/in_target_std': Array(330.141, dtype=float32), 'eval/episode_reward/upright_std': Array(214.664, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8626365661621094, 'eval/sps': 33137.98691839651}
 eval/walltime :  55.661505460739136
 training/sps :  19930.9089876584
 training/walltime :  210.0109794139862
 training/actor_loss :  -19.054838180541992
 training/alpha :  0.0015946596395224333
 training/alpha_loss :  5.2995869737060275e-06
 training/buffer_current_size :  3059392.0
 training/critic_loss :  0.029186662286520004
 eval/episode_reward :  273.5953063964844
 eval/episode_reward/in_target :  232.89894104003906
 eval/episode_reward/upright :  558.4698486328125
 eval/episode_reward_std :  291.43951416015625
 eval/episode_reward/in_target_std :  330.1407775878906
 eval/episode_reward/upright_std :  214.66436767578125
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.8626365661621094
 eval/sps :  33137.98691839651
[2025-07-07 15:51:24,946][absl][INFO] - step 3336704
[2025-07-07 15:51:56,639][absl][INFO] - {'eval/walltime': 59.46162557601929, 'training/sps': 19895.648613414873, 'training/walltime': 237.8940613269806, 'training/actor_loss': Array(-20.692, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(3.614e+06, dtype=float32), 'training/critic_loss': Array(0.027, dtype=float32), 'eval/episode_reward': Array(279.193, dtype=float32), 'eval/episode_reward/in_target': Array(229.234, dtype=float32), 'eval/episode_reward/upright': Array(628.91, dtype=float32), 'eval/episode_reward_std': Array(304.618, dtype=float32), 'eval/episode_reward/in_target_std': Array(345.042, dtype=float32), 'eval/episode_reward/upright_std': Array(210.988, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8001201152801514, 'eval/sps': 33683.145826184926}
 eval/walltime :  59.46162557601929
 training/sps :  19895.648613414873
 training/walltime :  237.8940613269806
 training/actor_loss :  -20.691741943359375
 training/alpha :  0.0015411053318530321
 training/alpha_loss :  -8.600714863860048e-06
 training/buffer_current_size :  3614144.0
 training/critic_loss :  0.026946045458316803
 eval/episode_reward :  279.193115234375
 eval/episode_reward/in_target :  229.23353576660156
 eval/episode_reward/upright :  628.9100952148438
 eval/episode_reward_std :  304.61810302734375
 eval/episode_reward/in_target_std :  345.041748046875
 eval/episode_reward/upright_std :  210.98776245117188
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.8001201152801514
 eval/sps :  33683.145826184926
[2025-07-07 15:51:56,645][absl][INFO] - step 3891456
[2025-07-07 15:52:37,953][absl][INFO] - {'eval/walltime': 63.26366448402405, 'training/sps': 14793.599607578986, 'training/walltime': 275.3935215473175, 'training/actor_loss': Array(-22.297, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(4.112e+06, dtype=float32), 'training/critic_loss': Array(0.025, dtype=float32), 'eval/episode_reward': Array(280.029, dtype=float32), 'eval/episode_reward/in_target': Array(240.43, dtype=float32), 'eval/episode_reward/upright': Array(557.22, dtype=float32), 'eval/episode_reward_std': Array(302.608, dtype=float32), 'eval/episode_reward/in_target_std': Array(348.445, dtype=float32), 'eval/episode_reward/upright_std': Array(234.91, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8020389080047607, 'eval/sps': 33666.146795739136}
 eval/walltime :  63.26366448402405
 training/sps :  14793.599607578986
 training/walltime :  275.3935215473175
 training/actor_loss :  -22.29667854309082
 training/alpha :  0.0015524097252637148
 training/alpha_loss :  2.633547182995244e-06
 training/buffer_current_size :  4111674.25
 training/critic_loss :  0.025107795372605324
 eval/episode_reward :  280.0289611816406
 eval/episode_reward/in_target :  240.43026733398438
 eval/episode_reward/upright :  557.2199096679688
 eval/episode_reward_std :  302.6076354980469
 eval/episode_reward/in_target_std :  348.44476318359375
 eval/episode_reward/upright_std :  234.9104461669922
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.8020389080047607
 eval/sps :  33666.146795739136
[2025-07-07 15:52:37,965][absl][INFO] - step 4446208
[2025-07-07 15:53:30,303][absl][INFO] - {'eval/walltime': 67.08635640144348, 'training/sps': 11435.973520976473, 'training/walltime': 323.902902841568, 'training/actor_loss': Array(-25.038, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.023, dtype=float32), 'eval/episode_reward': Array(279.79, dtype=float32), 'eval/episode_reward/in_target': Array(230.767, dtype=float32), 'eval/episode_reward/upright': Array(622.946, dtype=float32), 'eval/episode_reward_std': Array(321.448, dtype=float32), 'eval/episode_reward/in_target_std': Array(354.074, dtype=float32), 'eval/episode_reward/upright_std': Array(225.313, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8226919174194336, 'eval/sps': 33484.25736762181}
 eval/walltime :  67.08635640144348
 training/sps :  11435.973520976473
 training/walltime :  323.902902841568
 training/actor_loss :  -25.03812026977539
 training/alpha :  0.0015875225653871894
 training/alpha_loss :  -6.342143933579791e-06
 training/buffer_current_size :  4194304.0
 training/critic_loss :  0.022890152409672737
 eval/episode_reward :  279.78961181640625
 eval/episode_reward/in_target :  230.76727294921875
 eval/episode_reward/upright :  622.9458618164062
 eval/episode_reward_std :  321.4479064941406
 eval/episode_reward/in_target_std :  354.0743408203125
 eval/episode_reward/upright_std :  225.31321716308594
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  3.8226919174194336
 eval/sps :  33484.25736762181
[2025-07-07 15:53:31,461][absl][INFO] - total steps: 5000960
time to jit: 0:00:52.465751
time to train: 0:05:58.590602
Saving parameters to /home/sukchul/distributionally_robust_learning/learning/logs/FishSwim/3/sac/models
