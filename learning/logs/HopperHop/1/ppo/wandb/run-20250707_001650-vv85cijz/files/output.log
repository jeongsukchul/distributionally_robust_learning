training with ppo
INFO:2025-07-07 00:16:52,047:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 00:16:52,047][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 00:16:52,326][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 00:16:52,327][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 00:16:53,924][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-07 00:16:55,156][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 00:16:55,156][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 00:17:28,208][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 00:17:28,209][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 00:18:11,932][absl][INFO] - {'eval/walltime': 43.613654136657715, 'eval/episode_reward': Array(0.043, dtype=float32), 'eval/episode_reward/hopping': Array(28.696, dtype=float32), 'eval/episode_reward/standing': Array(1.273, dtype=float32), 'eval/episode_reward_std': Array(0.235, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.736, dtype=float32), 'eval/episode_reward/standing_std': Array(3.954, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 43.613654136657715, 'eval/sps': 2934.860711256357}
 eval/walltime :  43.613654136657715
 eval/episode_reward :  0.042676739394664764
 eval/episode_reward/hopping :  28.69647216796875
 eval/episode_reward/standing :  1.2734375
 eval/episode_reward_std :  0.2348880171775818
 eval/episode_reward/hopping_std :  6.736486434936523
 eval/episode_reward/standing_std :  3.9542596340179443
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  43.613654136657715
 eval/sps :  2934.860711256357
[2025-07-07 00:18:11,943][absl][INFO] - starting iteration 0 78.01906085014343
[2025-07-07 01:41:48,499][absl][INFO] - {'eval/walltime': 52.64236044883728, 'training/sps': np.float64(1374.2909819577565), 'training/walltime': 5007.149206638336, 'training/entropy_loss': Array(-0.025, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(-0.003, dtype=float32), 'training/v_loss': Array(0.023, dtype=float32), 'eval/episode_reward': Array(0.213, dtype=float32), 'eval/episode_reward/hopping': Array(28.574, dtype=float32), 'eval/episode_reward/standing': Array(4.289, dtype=float32), 'eval/episode_reward_std': Array(1.257, dtype=float32), 'eval/episode_reward/hopping_std': Array(5.889, dtype=float32), 'eval/episode_reward/standing_std': Array(9.189, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 9.028706312179565, 'eval/sps': 14177.003390544474}
 eval/walltime :  52.64236044883728
 training/sps :  1374.2909819577565
 training/walltime :  5007.149206638336
 training/entropy_loss :  -0.02531428448855877
 training/policy_loss :  -0.0015036211116239429
 training/total_loss :  -0.003372530685737729
 training/v_loss :  0.023445377126336098
 eval/episode_reward :  0.21283304691314697
 eval/episode_reward/hopping :  28.573835372924805
 eval/episode_reward/standing :  4.2890625
 eval/episode_reward_std :  1.2569363117218018
 eval/episode_reward/hopping_std :  5.8890581130981445
 eval/episode_reward/standing_std :  9.189117431640625
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  9.028706312179565
 eval/sps :  14177.003390544474
[2025-07-07 01:41:48,515][absl][INFO] - starting iteration 1 5094.591333150864
[2025-07-07 02:10:31,653][absl][INFO] - {'eval/walltime': 61.42512559890747, 'training/sps': np.float64(4013.9501876145137), 'training/walltime': 6721.490361452103, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.002, dtype=float32), 'training/total_loss': Array(0.068, dtype=float32), 'training/v_loss': Array(0.097, dtype=float32), 'eval/episode_reward': Array(1.219, dtype=float32), 'eval/episode_reward/hopping': Array(30.802, dtype=float32), 'eval/episode_reward/standing': Array(7.969, dtype=float32), 'eval/episode_reward_std': Array(2.816, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.233, dtype=float32), 'eval/episode_reward/standing_std': Array(13.799, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.78276515007019, 'eval/sps': 14573.997802841972}
 eval/walltime :  61.42512559890747
 training/sps :  4013.9501876145137
 training/walltime :  6721.490361452103
 training/entropy_loss :  -0.026663590222597122
 training/policy_loss :  -0.0020470195449888706
 training/total_loss :  0.06846698373556137
 training/v_loss :  0.09717757999897003
 eval/episode_reward :  1.219337821006775
 eval/episode_reward/hopping :  30.80207633972168
 eval/episode_reward/standing :  7.96875
 eval/episode_reward_std :  2.8162739276885986
 eval/episode_reward/hopping_std :  6.232781410217285
 eval/episode_reward/standing_std :  13.799307823181152
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.78276515007019
 eval/sps :  14573.997802841972
[2025-07-07 02:10:31,667][absl][INFO] - starting iteration 2 6817.743040323257
[2025-07-07 02:38:54,949][absl][INFO] - {'eval/walltime': 70.26432204246521, 'training/sps': np.float64(4061.1176921798856), 'training/walltime': 8415.920447349548, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.092, dtype=float32), 'training/v_loss': Array(0.12, dtype=float32), 'eval/episode_reward': Array(0.813, dtype=float32), 'eval/episode_reward/hopping': Array(31.198, dtype=float32), 'eval/episode_reward/standing': Array(8.469, dtype=float32), 'eval/episode_reward_std': Array(2.598, dtype=float32), 'eval/episode_reward/hopping_std': Array(7.176, dtype=float32), 'eval/episode_reward/standing_std': Array(14.505, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.83919644355774, 'eval/sps': 14480.954328522712}
 eval/walltime :  70.26432204246521
 training/sps :  4061.1176921798856
 training/walltime :  8415.920447349548
 training/entropy_loss :  -0.026764053851366043
 training/policy_loss :  -0.001178850419819355
 training/total_loss :  0.09225018322467804
 training/v_loss :  0.12019308656454086
 eval/episode_reward :  0.8129959106445312
 eval/episode_reward/hopping :  31.197521209716797
 eval/episode_reward/standing :  8.46875
 eval/episode_reward_std :  2.5983264446258545
 eval/episode_reward/hopping_std :  7.176395893096924
 eval/episode_reward/standing_std :  14.505352973937988
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.83919644355774
 eval/sps :  14480.954328522712
[2025-07-07 02:38:54,959][absl][INFO] - starting iteration 3 8521.0354013443
[2025-07-07 03:05:56,379][absl][INFO] - {'eval/walltime': 78.98343396186829, 'training/sps': np.float64(4266.960621199615), 'training/walltime': 10028.609340190887, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.051, dtype=float32), 'training/v_loss': Array(0.079, dtype=float32), 'eval/episode_reward': Array(0.663, dtype=float32), 'eval/episode_reward/hopping': Array(29.382, dtype=float32), 'eval/episode_reward/standing': Array(5.195, dtype=float32), 'eval/episode_reward_std': Array(2.607, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.686, dtype=float32), 'eval/episode_reward/standing_std': Array(12.551, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.719111919403076, 'eval/sps': 14680.394194178789}
 eval/walltime :  78.98343396186829
 training/sps :  4266.960621199615
 training/walltime :  10028.609340190887
 training/entropy_loss :  -0.026845622807741165
 training/policy_loss :  -0.0009421785944141448
 training/total_loss :  0.05144882947206497
 training/v_loss :  0.07923662662506104
 eval/episode_reward :  0.6630829572677612
 eval/episode_reward/hopping :  29.382205963134766
 eval/episode_reward/standing :  5.1953125
 eval/episode_reward_std :  2.607254981994629
 eval/episode_reward/hopping_std :  6.685789108276367
 eval/episode_reward/standing_std :  12.550559043884277
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.719111919403076
 eval/sps :  14680.394194178789
[2025-07-07 03:05:56,395][absl][INFO] - starting iteration 4 10142.471567630768
[2025-07-07 03:39:36,223][absl][INFO] - {'eval/walltime': 88.72107362747192, 'training/sps': np.float64(3423.391386317762), 'training/walltime': 12038.686256170273, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.035, dtype=float32), 'training/v_loss': Array(0.063, dtype=float32), 'eval/episode_reward': Array(0.704, dtype=float32), 'eval/episode_reward/hopping': Array(30.158, dtype=float32), 'eval/episode_reward/standing': Array(4.984, dtype=float32), 'eval/episode_reward_std': Array(2.3, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.97, dtype=float32), 'eval/episode_reward/standing_std': Array(10.788, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 9.737639665603638, 'eval/sps': 13144.869228642305}
 eval/walltime :  88.72107362747192
 training/sps :  3423.391386317762
 training/walltime :  12038.686256170273
 training/entropy_loss :  -0.0269294586032629
 training/policy_loss :  -0.0011403231183066964
 training/total_loss :  0.03541155159473419
 training/v_loss :  0.06348133087158203
 eval/episode_reward :  0.7041592001914978
 eval/episode_reward/hopping :  30.157512664794922
 eval/episode_reward/standing :  4.984375
 eval/episode_reward_std :  2.3004980087280273
 eval/episode_reward/hopping_std :  6.969580173492432
 eval/episode_reward/standing_std :  10.788436889648438
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  9.737639665603638
 eval/sps :  13144.869228642305
[2025-07-07 03:39:36,232][absl][INFO] - starting iteration 5 12162.308320760727
[2025-07-07 04:08:27,707][absl][INFO] - {'eval/walltime': 97.65499782562256, 'training/sps': np.float64(3994.8740105880556), 'training/walltime': 13761.213670492172, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.07, dtype=float32), 'training/v_loss': Array(0.098, dtype=float32), 'eval/episode_reward': Array(1.109, dtype=float32), 'eval/episode_reward/hopping': Array(30.87, dtype=float32), 'eval/episode_reward/standing': Array(6.125, dtype=float32), 'eval/episode_reward_std': Array(2.996, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.375, dtype=float32), 'eval/episode_reward/standing_std': Array(11.238, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.933924198150635, 'eval/sps': 14327.410571325041}
 eval/walltime :  97.65499782562256
 training/sps :  3994.8740105880556
 training/walltime :  13761.213670492172
 training/entropy_loss :  -0.02685438096523285
 training/policy_loss :  -0.0010119280777871609
 training/total_loss :  0.07004977762699127
 training/v_loss :  0.09791608154773712
 eval/episode_reward :  1.1091445684432983
 eval/episode_reward/hopping :  30.87044906616211
 eval/episode_reward/standing :  6.125
 eval/episode_reward_std :  2.9956557750701904
 eval/episode_reward/hopping_std :  6.375053882598877
 eval/episode_reward/standing_std :  11.238187789916992
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.933924198150635
 eval/sps :  14327.410571325041
[2025-07-07 04:08:27,718][absl][INFO] - starting iteration 6 13893.794706821442
[2025-07-07 04:40:04,435][absl][INFO] - {'eval/walltime': 106.45702505111694, 'training/sps': np.float64(3644.934073546853), 'training/walltime': 15649.116129398346, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.03, dtype=float32), 'training/v_loss': Array(0.058, dtype=float32), 'eval/episode_reward': Array(0.935, dtype=float32), 'eval/episode_reward/hopping': Array(29.591, dtype=float32), 'eval/episode_reward/standing': Array(6.273, dtype=float32), 'eval/episode_reward_std': Array(2.895, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.302, dtype=float32), 'eval/episode_reward/standing_std': Array(13.954, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.802027225494385, 'eval/sps': 14542.104531244575}
 eval/walltime :  106.45702505111694
 training/sps :  3644.934073546853
 training/walltime :  15649.116129398346
 training/entropy_loss :  -0.026933282613754272
 training/policy_loss :  -0.001041440642438829
 training/total_loss :  0.030168402940034866
 training/v_loss :  0.05814312770962715
 eval/episode_reward :  0.9352092742919922
 eval/episode_reward/hopping :  29.59145164489746
 eval/episode_reward/standing :  6.2734375
 eval/episode_reward_std :  2.894732713699341
 eval/episode_reward/hopping_std :  6.301599502563477
 eval/episode_reward/standing_std :  13.95400619506836
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.802027225494385
 eval/sps :  14542.104531244575
[2025-07-07 04:40:04,454][absl][INFO] - starting iteration 7 15790.530614614487
[2025-07-07 05:06:25,538][absl][INFO] - {'eval/walltime': 115.55727648735046, 'training/sps': np.float64(4377.478858814095), 'training/walltime': 17221.089455127716, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.04, dtype=float32), 'training/v_loss': Array(0.068, dtype=float32), 'eval/episode_reward': Array(0.654, dtype=float32), 'eval/episode_reward/hopping': Array(29.211, dtype=float32), 'eval/episode_reward/standing': Array(4.305, dtype=float32), 'eval/episode_reward_std': Array(2.4, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.308, dtype=float32), 'eval/episode_reward/standing_std': Array(10.651, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 9.10025143623352, 'eval/sps': 14065.5454299159}
 eval/walltime :  115.55727648735046
 training/sps :  4377.478858814095
 training/walltime :  17221.089455127716
 training/entropy_loss :  -0.026936229318380356
 training/policy_loss :  -0.001018798560835421
 training/total_loss :  0.03962705656886101
 training/v_loss :  0.06758208572864532
 eval/episode_reward :  0.6535587310791016
 eval/episode_reward/hopping :  29.210556030273438
 eval/episode_reward/standing :  4.3046875
 eval/episode_reward_std :  2.3995280265808105
 eval/episode_reward/hopping_std :  6.308329105377197
 eval/episode_reward/standing_std :  10.651113510131836
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  9.10025143623352
 eval/sps :  14065.5454299159
[2025-07-07 05:06:25,551][absl][INFO] - starting iteration 8 17371.62707734108
[2025-07-07 05:39:10,332][absl][INFO] - {'eval/walltime': 125.1017758846283, 'training/sps': np.float64(3519.4349661487026), 'training/walltime': 19176.312400341034, 'training/entropy_loss': Array(-0.027, dtype=float32), 'training/policy_loss': Array(-0.001, dtype=float32), 'training/total_loss': Array(0.041, dtype=float32), 'training/v_loss': Array(0.069, dtype=float32), 'eval/episode_reward': Array(0.919, dtype=float32), 'eval/episode_reward/hopping': Array(29.636, dtype=float32), 'eval/episode_reward/standing': Array(7.109, dtype=float32), 'eval/episode_reward_std': Array(2.964, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.021, dtype=float32), 'eval/episode_reward/standing_std': Array(17.759, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 9.544499397277832, 'eval/sps': 13410.865742891307}
 eval/walltime :  125.1017758846283
 training/sps :  3519.4349661487026
 training/walltime :  19176.312400341034
 training/entropy_loss :  -0.026828357949852943
 training/policy_loss :  -0.0009315303177572787
 training/total_loss :  0.04124747961759567
 training/v_loss :  0.06900736689567566
 eval/episode_reward :  0.9193097352981567
 eval/episode_reward/hopping :  29.635704040527344
 eval/episode_reward/standing :  7.109375
 eval/episode_reward_std :  2.9642155170440674
 eval/episode_reward/hopping_std :  6.020816326141357
 eval/episode_reward/standing_std :  17.75934410095215
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  9.544499397277832
 eval/sps :  13410.865742891307
[2025-07-07 05:39:11,039][absl][INFO] - total steps: 61931520
time to jit: 0:01:18.012969
time to train: 5:20:58.411508
Error executing job with overrides: ['task=HopperHop']
Traceback (most recent call last):
  File "/home/ros/mujoco_playground/learning/train.py", line 208, in train
    save_dir = make_dir(cfg.wrok_dir / "models")
                        ^^^^^^^^^^^^
omegaconf.errors.ConfigAttributeError: Key 'wrok_dir' is not in struct
    full_key: wrok_dir
    object_type=dict. Did you mean: 'work_dir'?

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
