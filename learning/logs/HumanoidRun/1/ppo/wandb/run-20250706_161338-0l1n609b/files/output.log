INFO:2025-07-06 16:13:40,488:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 16:13:40,488][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 16:13:40,755][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 16:13:40,756][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 16:13:41,587][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 16:13:42,491][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 16:13:42,492][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 16:14:23,116][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 16:14:23,117][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 16:16:06,271][absl][INFO] - {'eval/walltime': 103.01587104797363, 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(190.063, dtype=float32), 'eval/episode_reward/small_control': Array(890.391, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(617.22, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(12.888, dtype=float32), 'eval/episode_reward/small_control_std': Array(60.917, dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(72.587, dtype=float32), 'eval/avg_episode_length': Array(993.969, dtype=float32), 'eval/std_episode_length': Array(67.969, dtype=float32), 'eval/epoch_eval_time': 103.01587104797363, 'eval/sps': 1242.5269882967011}
[2025-07-06 16:16:06,293][absl][INFO] - starting iteration 0 144.70592284202576
[2025-07-06 16:21:09,537][absl][INFO] - {'eval/walltime': 118.56069731712341, 'training/sps': np.float64(23953.963797635017), 'training/walltime': 287.2710361480713, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.54482626914978, 'eval/sps': 8234.250919486212}
[2025-07-06 16:21:09,546][absl][INFO] - starting iteration 1 447.9591534137726
[2025-07-06 16:24:53,578][absl][INFO] - {'eval/walltime': 134.31025290489197, 'training/sps': np.float64(33040.99663085023), 'training/walltime': 495.53594040870667, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.749555587768555, 'eval/sps': 8127.213449718389}
[2025-07-06 16:24:53,601][absl][INFO] - starting iteration 2 672.014297246933
[2025-07-06 16:28:42,166][absl][INFO] - {'eval/walltime': 149.6149709224701, 'training/sps': np.float64(32269.023314990733), 'training/walltime': 708.7831754684448, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.304718017578125, 'eval/sps': 8363.434063468958}
[2025-07-06 16:28:42,185][absl][INFO] - starting iteration 3 900.5982277393341
[2025-07-06 16:32:32,750][absl][INFO] - {'eval/walltime': 166.05249524116516, 'training/sps': np.float64(32139.082673305256), 'training/walltime': 922.8925845623016, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.43752431869507, 'eval/sps': 7787.060722667365}
[2025-07-06 16:32:32,773][absl][INFO] - starting iteration 4 1131.1860477924347
[2025-07-06 16:36:25,165][absl][INFO] - {'eval/walltime': 182.29486727714539, 'training/sps': np.float64(31838.29504075861), 'training/walltime': 1139.0247609615326, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.242372035980225, 'eval/sps': 7880.622344843071}
[2025-07-06 16:36:25,190][absl][INFO] - starting iteration 5 1363.6027941703796
[2025-07-06 16:40:16,477][absl][INFO] - {'eval/walltime': 199.28912258148193, 'training/sps': np.float64(32113.572650211256), 'training/walltime': 1353.304251909256, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.994255304336548, 'eval/sps': 7531.956988273402}
[2025-07-06 16:40:16,492][absl][INFO] - starting iteration 6 1594.9050679206848
[2025-07-06 16:44:06,904][absl][INFO] - {'eval/walltime': 215.0375895500183, 'training/sps': np.float64(32058.100623935912), 'training/walltime': 1567.9545233249664, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.748466968536377, 'eval/sps': 8127.775246678249}
[2025-07-06 16:44:06,913][absl][INFO] - starting iteration 7 1825.3263158798218
[2025-07-06 16:47:58,860][absl][INFO] - {'eval/walltime': 231.7275516986847, 'training/sps': np.float64(31970.088152974393), 'training/walltime': 1783.1957190036774, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.689962148666382, 'eval/sps': 7669.2804249545825}
[2025-07-06 16:47:58,880][absl][INFO] - starting iteration 8 2057.2933247089386
[2025-07-06 16:51:49,550][absl][INFO] - {'eval/walltime': 248.44410872459412, 'training/sps': np.float64(32165.359098287066), 'training/walltime': 1997.1302185058594, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.716557025909424, 'eval/sps': 7657.07913427445}
Error executing job with overrides: ['task=HumanoidRun']
Traceback (most recent call last):
  File "/home/ros/mujoco_playground/learning/train.py", line 209, in train
  File "/home/ros/mujoco_playground/learning/train.py", line 139, in train_ppo
    **sac_params.network_factory

  File "/home/ros/miniconda3/envs/mujoco/lib/python3.12/site-packages/brax/training/agents/ppo/train.py", line 754, in train
    pmap.assert_is_replicated(training_state)
  File "/home/ros/miniconda3/envs/mujoco/lib/python3.12/site-packages/brax/training/pmap.py", line 70, in assert_is_replicated
    assert jax.pmap(f, axis_name='i')(x)[0], debug
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
AssertionError: None

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
