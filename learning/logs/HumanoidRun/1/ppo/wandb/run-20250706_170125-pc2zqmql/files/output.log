INFO:2025-07-06 17:01:27,139:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 17:01:27,139][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 17:01:27,549][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 17:01:27,549][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 17:01:28,150][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 17:01:28,893][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 17:01:28,893][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 17:02:03,694][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 17:02:03,694][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 17:03:26,951][absl][INFO] - {'eval/walltime': 83.13934469223022, 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(191.048, dtype=float32), 'eval/episode_reward/small_control': Array(894.908, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(611.714, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(3., dtype=float32), 'eval/episode_reward/small_control_std': Array(11.293, dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(63.934, dtype=float32), 'eval/avg_episode_length': Array(998.883, dtype=float32), 'eval/std_episode_length': Array(12.59, dtype=float32), 'eval/epoch_eval_time': 83.13934469223022, 'eval/sps': 1539.5839415602493}
[2025-07-06 17:03:26,963][absl][INFO] - starting iteration 0 118.81288433074951
[2025-07-06 17:08:19,839][absl][INFO] - {'eval/walltime': 98.92558145523071, 'training/sps': np.float64(24869.744453261308), 'training/walltime': 276.6928310394287, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.786236763000488, 'eval/sps': 8108.328914716661}
[2025-07-06 17:08:19,853][absl][INFO] - starting iteration 1 411.70259952545166
[2025-07-06 17:12:06,597][absl][INFO] - {'eval/walltime': 114.58685827255249, 'training/sps': np.float64(32602.453178331547), 'training/walltime': 487.759156703949, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.661276817321777, 'eval/sps': 8173.024555598729}
[2025-07-06 17:12:06,609][absl][INFO] - starting iteration 2 638.458420753479
[2025-07-06 17:15:51,371][absl][INFO] - {'eval/walltime': 130.4905879497528, 'training/sps': np.float64(32949.28187152962), 'training/walltime': 696.6037690639496, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.903729677200317, 'eval/sps': 8048.426538807534}
[2025-07-06 17:15:51,395][absl][INFO] - starting iteration 3 863.2442736625671
[2025-07-06 17:19:37,499][absl][INFO] - {'eval/walltime': 145.95931720733643, 'training/sps': np.float64(32671.969532517975), 'training/walltime': 907.2210075855255, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.468729257583618, 'eval/sps': 8274.758570568903}
[2025-07-06 17:19:37,518][absl][INFO] - starting iteration 4 1089.3677229881287
[2025-07-06 17:23:23,876][absl][INFO] - {'eval/walltime': 161.4643316268921, 'training/sps': np.float64(32637.801234349365), 'training/walltime': 1118.058739900589, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.505014419555664, 'eval/sps': 8255.393805926442}
[2025-07-06 17:23:23,893][absl][INFO] - starting iteration 5 1315.742518901825
[2025-07-06 17:27:10,449][absl][INFO] - {'eval/walltime': 177.28082036972046, 'training/sps': np.float64(32655.307536622884), 'training/walltime': 1328.7834434509277, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.81648874282837, 'eval/sps': 8092.820225856938}
[2025-07-06 17:27:10,465][absl][INFO] - starting iteration 6 1542.3147897720337
[2025-07-06 17:30:55,935][absl][INFO] - {'eval/walltime': 193.46137118339539, 'training/sps': np.float64(32881.42537853702), 'training/walltime': 1538.0590426921844, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.180550813674927, 'eval/sps': 7910.731932056437}
[2025-07-06 17:30:55,956][absl][INFO] - starting iteration 7 1767.8058516979218
[2025-07-06 17:34:40,992][absl][INFO] - {'eval/walltime': 209.30548405647278, 'training/sps': np.float64(32897.10578710344), 'training/walltime': 1747.2348906993866, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.844112873077393, 'eval/sps': 8078.710434933845}
[2025-07-06 17:34:41,010][absl][INFO] - starting iteration 8 1992.8594188690186
[2025-07-06 17:38:25,841][absl][INFO] - {'eval/walltime': 225.0879156589508, 'training/sps': np.float64(32919.227423564924), 'training/walltime': 1956.270173072815, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.782431602478027, 'eval/sps': 8110.283841173276}
Error executing job with overrides: ['task=HumanoidRun']
Traceback (most recent call last):
  File "/home/ros/mujoco_playground/learning/train.py", line 195, in train
    make_inference_fn, params, metrics = train_ppo(cfg)
                                         ^^^^^^^^^^^^^^
  File "/home/ros/mujoco_playground/learning/train.py", line 123, in train_ppo
    make_inference_fn, params, metrics = train_fn(
                                         ^^^^^^^^^
  File "/home/ros/miniconda3/envs/mujoco/lib/python3.12/site-packages/brax/training/agents/ppo/train.py", line 754, in train
    pmap.assert_is_replicated(training_state)
  File "/home/ros/miniconda3/envs/mujoco/lib/python3.12/site-packages/brax/training/pmap.py", line 70, in assert_is_replicated
    assert jax.pmap(f, axis_name='i')(x)[0], debug
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
AssertionError: None

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
