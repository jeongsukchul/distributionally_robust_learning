training with ppo
INFO:2025-07-07 10:52:17,596:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 10:52:17,596][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 10:52:17,768][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 10:52:17,769][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 10:52:18,071][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-07 10:52:19,221][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 10:52:19,222][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 10:52:59,304][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 10:52:59,305][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 10:54:31,430][absl][INFO] - {'eval/walltime': 91.98755550384521, 'eval/episode_reward': Array(1.726, dtype=float32), 'eval/episode_reward/move': Array(191.442, dtype=float32), 'eval/episode_reward/small_control': Array(895.867, dtype=float32), 'eval/episode_reward/stand': Array(11.077, dtype=float32), 'eval/episode_reward/standing': Array(11.126, dtype=float32), 'eval/episode_reward/upright': Array(610.897, dtype=float32), 'eval/episode_reward_std': Array(0.155, dtype=float32), 'eval/episode_reward/move_std': Array(1.787, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.665, dtype=float32), 'eval/episode_reward/stand_std': Array(0.695, dtype=float32), 'eval/episode_reward/standing_std': Array(0.698, dtype=float32), 'eval/episode_reward/upright_std': Array(64.62, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 91.98755550384521, 'eval/sps': 1391.4925698253762}
 eval/walltime :  91.98755550384521
 eval/episode_reward :  1.7264760732650757
 eval/episode_reward/move :  191.4420166015625
 eval/episode_reward/small_control :  895.8673095703125
 eval/episode_reward/stand :  11.07710075378418
 eval/episode_reward/standing :  11.126466751098633
 eval/episode_reward/upright :  610.8973999023438
 eval/episode_reward_std :  0.15516269207000732
 eval/episode_reward/move_std :  1.7867045402526855
 eval/episode_reward/small_control_std :  1.6651023626327515
 eval/episode_reward/stand_std :  0.6949856877326965
 eval/episode_reward/standing_std :  0.6976438760757446
 eval/episode_reward/upright_std :  64.6201400756836
 eval/avg_episode_length :  1000.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  91.98755550384521
 eval/sps :  1391.4925698253762
[2025-07-07 10:54:31,438][absl][INFO] - starting iteration 0 133.36639833450317
[2025-07-07 10:57:03,585][absl][INFO] - {'eval/walltime': 99.91562700271606, 'training/sps': np.float64(47902.52080972042), 'training/walltime': 143.65173029899597, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 7.92807149887085, 'eval/sps': 16145.162164371288}
 eval/walltime :  99.91562700271606
 training/sps :  47902.52080972042
 training/walltime :  143.65173029899597
 training/entropy_loss :  nan
 training/policy_loss :  nan
 training/total_loss :  nan
 training/v_loss :  nan
 eval/episode_reward :  nan
 eval/episode_reward/move :  0.1666666716337204
 eval/episode_reward/small_control :  0.800000011920929
 eval/episode_reward/stand :  nan
 eval/episode_reward/standing :  nan
 eval/episode_reward/upright :  0.0
 eval/episode_reward_std :  nan
 eval/episode_reward/move_std :  0.0
 eval/episode_reward/small_control_std :  0.0
 eval/episode_reward/stand_std :  nan
 eval/episode_reward/standing_std :  nan
 eval/episode_reward/upright_std :  0.0
 eval/avg_episode_length :  1.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  7.92807149887085
 eval/sps :  16145.162164371288
[2025-07-07 10:57:03,600][absl][INFO] - starting iteration 1 285.5280923843384
[2025-07-07 10:58:26,509][absl][INFO] - {'eval/walltime': 107.74986386299133, 'training/sps': np.float64(91680.65459997919), 'training/walltime': 218.7087860107422, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 7.8342368602752686, 'eval/sps': 16338.54098145081}
 eval/walltime :  107.74986386299133
 training/sps :  91680.65459997919
 training/walltime :  218.7087860107422
 training/entropy_loss :  nan
 training/policy_loss :  nan
 training/total_loss :  nan
 training/v_loss :  nan
 eval/episode_reward :  nan
 eval/episode_reward/move :  0.1666666716337204
 eval/episode_reward/small_control :  0.800000011920929
 eval/episode_reward/stand :  nan
 eval/episode_reward/standing :  nan
 eval/episode_reward/upright :  0.0
 eval/episode_reward_std :  nan
 eval/episode_reward/move_std :  0.0
 eval/episode_reward/small_control_std :  0.0
 eval/episode_reward/stand_std :  nan
 eval/episode_reward/standing_std :  nan
 eval/episode_reward/upright_std :  0.0
 eval/avg_episode_length :  1.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  7.8342368602752686
 eval/sps :  16338.54098145081
[2025-07-07 10:58:26,515][absl][INFO] - starting iteration 2 368.4435365200043
[2025-07-07 10:59:48,675][absl][INFO] - {'eval/walltime': 115.6474871635437, 'training/sps': np.float64(92675.32670407803), 'training/walltime': 292.9602642059326, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 7.897623300552368, 'eval/sps': 16207.407612242983}
 eval/walltime :  115.6474871635437
 training/sps :  92675.32670407803
 training/walltime :  292.9602642059326
 training/entropy_loss :  nan
 training/policy_loss :  nan
 training/total_loss :  nan
 training/v_loss :  nan
 eval/episode_reward :  nan
 eval/episode_reward/move :  0.1666666716337204
 eval/episode_reward/small_control :  0.800000011920929
 eval/episode_reward/stand :  nan
 eval/episode_reward/standing :  nan
 eval/episode_reward/upright :  0.0
 eval/episode_reward_std :  nan
 eval/episode_reward/move_std :  0.0
 eval/episode_reward/small_control_std :  0.0
 eval/episode_reward/stand_std :  nan
 eval/episode_reward/standing_std :  nan
 eval/episode_reward/upright_std :  0.0
 eval/avg_episode_length :  1.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  7.897623300552368
 eval/sps :  16207.407612242983
[2025-07-07 10:59:48,682][absl][INFO] - starting iteration 3 450.610417842865
[2025-07-07 11:01:10,970][absl][INFO] - {'eval/walltime': 123.46341252326965, 'training/sps': np.float64(92422.47141467374), 'training/walltime': 367.41488432884216, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 7.815925359725952, 'eval/sps': 16376.81964819685}
 eval/walltime :  123.46341252326965
 training/sps :  92422.47141467374
 training/walltime :  367.41488432884216
 training/entropy_loss :  nan
 training/policy_loss :  nan
 training/total_loss :  nan
 training/v_loss :  nan
 eval/episode_reward :  nan
 eval/episode_reward/move :  0.1666666716337204
 eval/episode_reward/small_control :  0.800000011920929
 eval/episode_reward/stand :  nan
 eval/episode_reward/standing :  nan
 eval/episode_reward/upright :  0.0
 eval/episode_reward_std :  nan
 eval/episode_reward/move_std :  0.0
 eval/episode_reward/small_control_std :  0.0
 eval/episode_reward/stand_std :  nan
 eval/episode_reward/standing_std :  nan
 eval/episode_reward/upright_std :  0.0
 eval/avg_episode_length :  1.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  7.815925359725952
 eval/sps :  16376.81964819685
[2025-07-07 11:01:10,984][absl][INFO] - starting iteration 4 532.9121644496918
[2025-07-07 11:02:34,633][absl][INFO] - {'eval/walltime': 131.98009824752808, 'training/sps': np.float64(91611.92705415229), 'training/walltime': 442.52824807167053, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 8.516685724258423, 'eval/sps': 15029.320576596174}
 eval/walltime :  131.98009824752808
 training/sps :  91611.92705415229
 training/walltime :  442.52824807167053
 training/entropy_loss :  nan
 training/policy_loss :  nan
 training/total_loss :  nan
 training/v_loss :  nan
 eval/episode_reward :  nan
 eval/episode_reward/move :  0.1666666716337204
 eval/episode_reward/small_control :  0.800000011920929
 eval/episode_reward/stand :  nan
 eval/episode_reward/standing :  nan
 eval/episode_reward/upright :  0.0
 eval/episode_reward_std :  nan
 eval/episode_reward/move_std :  0.0
 eval/episode_reward/small_control_std :  0.0
 eval/episode_reward/stand_std :  nan
 eval/episode_reward/standing_std :  nan
 eval/episode_reward/upright_std :  0.0
 eval/avg_episode_length :  1.0
 eval/std_episode_length :  0.0
 eval/epoch_eval_time :  8.516685724258423
 eval/sps :  15029.320576596174
[2025-07-07 11:02:34,639][absl][INFO] - starting iteration 5 616.5673720836639
Traceback (most recent call last):
  File "/home/sukchul/distributionally_robust_learning/learning/train.py", line 214, in <module>
    train()
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
          ^^^^^^^^
  File "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sukchul/distributionally_robust_learning/learning/train.py", line 203, in train
    make_inference_fn, params, metrics = train_ppo(cfg)
                                         ^^^^^^^^^^^^^^
  File "/home/sukchul/distributionally_robust_learning/learning/train.py", line 126, in train_ppo
    make_inference_fn, params, metrics = train_fn(
                                         ^^^^^^^^^
  File "/home/sukchul/distributionally_robust_learning/brax/brax/training/agents/ppo/train.py", line 702, in train
    training_epoch_with_timing(training_state, env_state, epoch_keys)
  File "/home/sukchul/distributionally_robust_learning/brax/brax/training/agents/ppo/train.py", line 575, in training_epoch_with_timing
    result = training_epoch(training_state, env_state, key)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in __init__
KeyboardInterrupt
