{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/sukchul/miniconda3/envs/mujoco/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import Callable, NamedTuple, Tuple\n",
    "proj = Path.home() / \"papercode/variational_sampling_methods\"\n",
    "sys.path.insert(0, str(proj))\n",
    "# os.environ['PYTHONPATH'] = os.environ.get(\"PYTHONPATH\",\"\") + \"~/papercode/variational_sampling_methods\"\n",
    "import hydra\n",
    "import jax\n",
    "import matplotlib\n",
    "import distrax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from learning.module.target_examples.funnel import Funnel\n",
    "from learning.module.target_examples.gmm40 import GMM40\n",
    "import functools\n",
    "import chex\n",
    "from omegaconf import OmegaConf\n",
    "os.environ['HYDRA_FULL_ERROR'] = '1'\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup GMMVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from learning.module.gmmvi.component_adaptation import ComponentAdaptationState, setup_vips_component_adaptation\n",
    "# from learning.module.gmmvi.configs import get_default_algorithm_config\n",
    "# from learning.module.gmmvi.gmm_setup import GMMWrapper, GMMWrapperState, setup_full_cov_gmm, setup_gmm_wrapper\n",
    "# from learning.module.gmmvi.ng_update import get_ng_update_fns\n",
    "# from learning.module.gmmvi.sample_db import SampleDB, SampleDBState, setup_sampledb\n",
    "# from learning.module.gmmvi.sample_selector import setup_fixed_sample_selector\n",
    "# from learning.module.gmmvi.stepsize_update import WeightStepsizeAdaptationState, update_component_stepsize_adaptive\n",
    "# from learning.module.gmmvi.weight_update import setup_weight_update_fn\n",
    "# num_initial_components = 10\n",
    "# prior_mean=0.\n",
    "# prior_scale = 10.\n",
    "# init_std = 10.\n",
    "# dim=2\n",
    "# config=get_default_algorithm_config('SAQTRUX')\n",
    "# seed = 0\n",
    "# initial_stepsize=1.0 # component stepsize and weight step size\n",
    "# initial_l2_regularizer=1e-5\n",
    "# use_diagonal_convs = False\n",
    "# use_self_normalized_importance_weights = True\n",
    "# temperature = 1.0\n",
    "# # sample db config\n",
    "# use_sample_database = False\n",
    "# max_database_size = int(1e6)\n",
    "# desire_samples_per_component = 100\n",
    "# # sample selector config\n",
    "# desired_samples_per_component = 100\n",
    "# ratio_reused_samples_to_desired = 2.0\n",
    "# #component stepsize config\n",
    "# min_stepsize = 1e-4\n",
    "# max_stepsize = 1.0\n",
    "# stepsize_inc_factor = 1.1\n",
    "# stepsize_dec_factor = 0.9\n",
    "# #num component adaptation\n",
    "# del_iters = 100\n",
    "# add_iters = 30\n",
    "# max_components = 1000\n",
    "# thresholds_for_add_heuristic =  [5000.0, 1000.0, 500.0, 200.0, 100.0, 50.0]\n",
    "# min_weight_for_del_heuristic = 1e-06\n",
    "# num_database_samples = int(1e6)\n",
    "\n",
    "\n",
    "# #Start Setting!\n",
    "# rng = jax.random.PRNGKey(seed)\n",
    "# gmm = setup_full_cov_gmm(dim)\n",
    "# gmm_state = gmm.init_gmm_state(rng,\n",
    "#                                 num_initial_components,\n",
    "#                                 prior_mean,\n",
    "#                                 prior_scale,\n",
    "#                                 use_diagonal_convs,\n",
    "#                                 init_std)\n",
    "# model = setup_gmm_wrapper(gmm,\n",
    "#                         initial_stepsize,\n",
    "#                         initial_l2_regularizer,\n",
    "#                         10000)\n",
    "# model_state = model.init_gmm_wrapper_state(gmm_state)\n",
    "# sample_db = setup_sampledb(dim,\n",
    "#                            use_sample_database,\n",
    "#                             max_database_size,\n",
    "#                             use_diagonal_convs,\n",
    "#                             desire_samples_per_component)\n",
    "# sample_db_state = sample_db.init_sampleDB_state()\n",
    "# # 'S' ; Stein Estimator +  'T' : Trust Rgeion Component Updater\n",
    "# ng_update_fn, hass_grad_fn = get_ng_update_fns(model,\n",
    "#                                         dim,\n",
    "#                                         use_diagonal_convs,\n",
    "#                                         use_self_normalized_importance_weights,\n",
    "#                                         temperature,\n",
    "#                                         initial_l2_regularizer,\n",
    "#                                         )\n",
    "# # 'A' vips component adaptation component increasing.\n",
    "# component_adapter_state, component_adapter = setup_vips_component_adaptation(sample_db,\n",
    "#                     model,\n",
    "#                     dim,\n",
    "#                     prior_mean,\n",
    "#                     init_std ** 2,\n",
    "#                     use_diagonal_convs,\n",
    "#                     del_iters,\n",
    "#                     add_iters,\n",
    "#                     max_components,\n",
    "#                     thresholds_for_add_heuristic,\n",
    "#                     min_weight_for_del_heuristic,\n",
    "#                     num_database_samples)\n",
    "# # 'Q' Fixed Sample Selector we can't use other because target log prob is expensive\n",
    "# sample_selector = setup_fixed_sample_selector(sample_db,\n",
    "#                                                 model,\n",
    "#                                                 desired_samples_per_component,\n",
    "#                                                 ratio_reused_samples_to_desired)\n",
    "# # 'R' Adaptive Component Stepsize update\n",
    "# component_stepsize_fn = functools.partial(update_component_stepsize_adaptive, \n",
    "#                                             MIN_STEPSIZE=min_stepsize,               \n",
    "#                                             MAX_STEPSIZE=max_stepsize,\n",
    "#                                             STEPSIZE_INC_FACTOR=stepsize_inc_factor,\n",
    "#                                             STEPSIZE_DEC_FACTOR=stepsize_dec_factor)\n",
    "# # 'U' Direct Update\n",
    "# weight_update_fn = setup_weight_update_fn(model,\n",
    "#                                         temperature,\n",
    "#                                         use_self_normalized_importance_weights)\n",
    "# # 'X' Weight Stepsize Adaptation\n",
    "# # Fxied so not implemented\n",
    "\n",
    "# class GMMTrainingState(NamedTuple):\n",
    "#     temperature: float\n",
    "#     num_updates: chex.Array\n",
    "#     model_state: GMMWrapperState\n",
    "#     sample_db_state: SampleDBState\n",
    "#     component_adaptation_state: ComponentAdaptationState\n",
    "# class GMMNetwork(NamedTuple):\n",
    "#     model : GMMWrapper\n",
    "#     sample_db : SampleDB\n",
    "#     ng_estimator : Callable\n",
    "#     component_adapter : Callable\n",
    "#     component_updater : Callable\n",
    "#     sample_selector : Callable\n",
    "#     component_stepsize_fn : Callable\n",
    "#     weight_updater : Callable\n",
    "# initial_train_state = GMMTrainingState(temperature=temperature,\n",
    "#                                 num_updates=jnp.array([0]),\n",
    "#                                 model_state=model_state,\n",
    "#                                 sample_db_state=sample_db_state,\n",
    "#                                 component_adaptation_state=component_adapter_state)\n",
    "# gmm_network = GMMNetwork(model = model,\n",
    "#            sample_db=sample_db,\n",
    "#           ng_estimator=hass_grad_fn, \n",
    "#           component_adapter=component_adapter,\n",
    "#           component_updater=ng_update_fn,\n",
    "#           sample_selector=sample_selector,\n",
    "#           component_stepsize_fn=component_stepsize_fn,\n",
    "#           weight_updater=weight_update_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.module.gmmvi.network import create_gmm_network_and_state\n",
    "\n",
    "\n",
    "dim=2\n",
    "key= jax.random.PRNGKey(0)\n",
    "batch_size=1024\n",
    "initial_train_state, gmm_network = create_gmm_network_and_state(dim, batch_size, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from learning.module.gmmvi.network import GMMTrainingState\n",
    "from learning.module.target_examples.student_t_mixture import StudentTMixtureModel\n",
    "\n",
    "target = StudentTMixtureModel(dim=dim, sample_bounds=[-30., 30.], num_components=40)\n",
    "target = GMM40(dim=dim)\n",
    "# target = Funnel(dim=dim, sample_bounds=[-30, 30])\n",
    "low = jnp.array([-target._plot_bound,-target._plot_bound])\n",
    "high = jnp.array([target._plot_bound, target._plot_bound])\n",
    "plot_bound = target._plot_bound\n",
    "def train_iter(train_state: GMMTrainingState, key: chex.Array, target_log_prob_fn):\n",
    "    def get_target_grads(samples: chex.Array) -> Tuple[chex.Array, chex.Array]:\n",
    "        # samples = jnp.where(jnp.logical_or(samples > high, samples < low) , \\\n",
    "        #                     jnp.clip(samples, low, high), samples)\n",
    "        samples = jnp.clip(samples, -plot_bound, plot_bound)\n",
    "                            # jax.lax.stop_gradient(jnp.clip(samples, low, high)), samples)\n",
    "        target, gradient = jax.vmap(jax.value_and_grad(target_log_prob_fn))(samples)\n",
    "        return gradient, target\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_samples, mapping, num_reused_samples = gmm_network.sample_selector.select_samples(train_state.model_state,\n",
    "                                        train_state.sample_db_state,\n",
    "                                        subkey)\n",
    "\n",
    "    # print(\"num samples\", train_state.sample_db_state.samples.shape)\n",
    "    # print('num_reused_samples', num_reused_samples)\n",
    "    new_target_grads, new_target_lnpdfs = get_target_grads(new_samples)\n",
    "    # new_target_grads, new_target_lnpdfs = get_target_grads(low+ (high-low) * jax.nn.sigmoid(new_samples))\n",
    "    new_sample_db_state, samples, mapping, sample_dist_densities, target_lnpdfs, target_lnpdf_grads =\\\n",
    "        gmm_network.sample_selector.save_samples_and_select(train_state.model_state,\n",
    "                                          train_state.sample_db_state,\n",
    "                                            new_samples,\n",
    "                                            new_target_lnpdfs,\n",
    "                                            new_target_grads,\n",
    "                                            mapping,\n",
    "                                            num_reused_samples)\n",
    "    # print(\"total sample for trainiing samples\", samples.shape)\n",
    "    new_component_stepsizes = gmm_network.component_stepsize_fn(train_state.model_state)\n",
    "    new_model_state = gmm_network.model.update_stepsizes(train_state.model_state, new_component_stepsizes)\n",
    "    expected_hessian_neg, expected_grad_neg = gmm_network.ng_estimator(new_model_state,\n",
    "                                                            samples,\n",
    "                                                            sample_dist_densities,\n",
    "                                                            target_lnpdfs,\n",
    "                                                            target_lnpdf_grads,\n",
    "                                                            int(train_state.model_state.gmm_state.num_components))\n",
    "    new_model_state = gmm_network.component_updater(new_model_state,\n",
    "                                    expected_hessian_neg,\n",
    "                                    expected_grad_neg,\n",
    "                                    new_model_state.stepsizes)\n",
    "\n",
    "    # new_weight_stepsize_adapter_state = weight_stepsize_adapter.update_stepsize(train_state.weight_stepsize_adapter_state, new_model_state)\n",
    "    new_model_state = gmm_network.weight_updater(new_model_state, samples, sample_dist_densities, target_lnpdfs,\n",
    "                                                    train_state.weight_stepsize)\n",
    "    new_num_updates = train_state.num_updates + 1\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_model_state, new_component_adapter_state, new_sample_db_state = \\\n",
    "        gmm_network.component_adapter(train_state.component_adaptation_state,\n",
    "                                                    new_sample_db_state,\n",
    "                                                    new_model_state,\n",
    "                                                    new_num_updates,\n",
    "                                                    subkey)\n",
    "\n",
    "    return GMMTrainingState(temperature=train_state.temperature,\n",
    "                        model_state=new_model_state,\n",
    "                        component_adaptation_state=new_component_adapter_state,\n",
    "                        num_updates=new_num_updates,\n",
    "                        sample_db_state=new_sample_db_state,\n",
    "                        weight_stepsize=train_state.weight_stepsize)\n",
    "def eval(seed: chex.Array, train_state: GMMTrainingState, target_log_prob_fn, n_eval_samples, target_samples=None):\n",
    "    samples = gmm_network.model.sample(train_state.model_state.gmm_state, seed, n_eval_samples)[0]\n",
    "    log_prob_model = jax.vmap(gmm_network.model.log_density, in_axes=(None, 0))(train_state.model_state.gmm_state, samples)\n",
    "    log_prob_target = jax.vmap(target_log_prob_fn)(samples)\n",
    "    log_ratio = log_prob_target - log_prob_model\n",
    "\n",
    "    if target_samples is not None:\n",
    "        fwd_log_prob_model = jax.vmap(gmm_network.model.log_density, in_axes=(None, 0))(train_state.model_state.gmm_state, target_samples)\n",
    "        fwd_log_prob_target = jax.vmap(target_log_prob_fn)(target_samples)\n",
    "        fwd_log_ratio = fwd_log_prob_target - fwd_log_prob_model\n",
    "    else:\n",
    "        fwd_log_ratio = None\n",
    "\n",
    "    return samples, log_ratio, log_prob_target, fwd_log_ratio, n_eval_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import wandb\n",
    "\n",
    "from learning.module.target_examples.student_t_mixture import StudentTMixtureModel\n",
    "\n",
    "\n",
    "logger = {\n",
    "    'KL/elbo': [],\n",
    "    'KL/eubo': [],\n",
    "    'logZ/delta_forward': [],\n",
    "    'logZ/forward': [],\n",
    "    'logZ/delta_reverse': [],\n",
    "    'logZ/reverse': [],\n",
    "    'ESS/forward': [],\n",
    "    'ESS/reverse': [],\n",
    "    'discrepancies/mmd': [],\n",
    "    'discrepancies/sd': [],\n",
    "    'other/target_log_prob': [],\n",
    "    'other/EMC': [],\n",
    "    \"stats/step\": [],\n",
    "    \"stats/wallclock\": [],\n",
    "    \"stats/nfe\": [],\n",
    "}\n",
    "def eval_fn(samples, log_ratio, target_log_prob, fwd_log_ratio, n_eval_samples):\n",
    "    ln_z = jax.nn.logsumexp(log_ratio) - jnp.log(n_eval_samples)\n",
    "    elbo = jnp.mean(log_ratio)\n",
    "\n",
    "    if target.log_Z is not None:\n",
    "        logger['logZ/delta_reverse'].append(jnp.abs(ln_z - target.log_Z))\n",
    "\n",
    "    logger['logZ/reverse'].append(ln_z)\n",
    "    logger['KL/elbo'].append(elbo)\n",
    "    # logger['ESS/reverse'].append(compute_reverse_ess(log_ratio, n_eval_samples))\n",
    "    logger['other/target_log_prob'].append(jnp.mean(target_log_prob))\n",
    "\n",
    "    # if cfg.compute_forward_metrics and (target_samples is not None):\n",
    "    #     eubo = jnp.mean(fwd_log_ratio)\n",
    "    #     fwd_ln_z = - (jax.scipy.special.logsumexp(-fwd_log_ratio) - jnp.log(cfg.eval_samples))\n",
    "    #     fwd_ess = jnp.exp(fwd_ln_z - (jax.scipy.special.logsumexp(fwd_log_ratio) - jnp.log(cfg.eval_samples)))\n",
    "\n",
    "    #     if target.log_Z is not None:\n",
    "    #         logger['logZ/delta_forward'].append(jnp.abs(fwd_ln_z - target.log_Z))\n",
    "    #     logger['logZ/forward'].append(fwd_ln_z)\n",
    "    #     logger['KL/eubo'].append(eubo)\n",
    "    #     logger['ESS/forward'].append(fwd_ess)\n",
    "\n",
    "    logger.update(target.visualise(samples=samples, show=True))\n",
    "\n",
    "    # if cfg.compute_emc and cfg.target.has_entropy:\n",
    "    #     logger['other/EMC'].append(target.entropy(samples))\n",
    "\n",
    "    # for d in cfg.discrepancies:\n",
    "    #     logger[f'discrepancies/{d}'].append(getattr(discrepancies, f'compute_{d}')(target_samples, samples,\n",
    "    #                                                                                 cfg) if target_samples is not None else jnp.inf)\n",
    "    # if cfg.moving_average.use_ma:\n",
    "    #     logger.update(moving_averages(logger, window_size=cfg.moving_average.window_size))\n",
    "\n",
    "    # if cfg.save_samples:\n",
    "    #     save_samples(cfg, logger, samples)\n",
    "\n",
    "    return logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape float32[]\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function train_per_freq at /tmp/ipykernel_3181388/4095047525.py:16 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:f32[]\u001b[39m = floor 0.0:f32[]\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:48:31 (setup_fixed_sample_selector.<locals>.select_samples)\n\n  operation a\u001b[35m:f32[]\u001b[39m = min 0.0:f32[] b\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:49:29 (setup_fixed_sample_selector.<locals>.select_samples)\n\n  operation c\u001b[35m:i32[1024]\u001b[39m = pjit[\n  name=_randint\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:key<fry>[]\u001b[39m d\u001b[35m:i32[]\u001b[39m b\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22me\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; f\u001b[35m:i32[]\u001b[39m g\u001b[35m:i32[]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = max g f\n            e\u001b[35m:i32[]\u001b[39m = min h i\n          \u001b[34;1min \u001b[39;22m(e,) }\n      ] 2147483647:i32[] -2147483648:i32[] 2147483647:i32[]\n      j\u001b[35m:bool[]\u001b[39m = gt b e\n      k\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; d\u001b[35m:i32[]\u001b[39m l\u001b[35m:i32[]\u001b[39m m\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:i32[]\u001b[39m = max l d\n            k\u001b[35m:i32[]\u001b[39m = min m n\n          \u001b[34;1min \u001b[39;22m(k,) }\n      ] d -2147483648:i32[] 2147483647:i32[]\n      o\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] k\n      p\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; d\u001b[35m:i32[]\u001b[39m l\u001b[35m:i32[]\u001b[39m m\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:i32[]\u001b[39m = max l d\n            k\u001b[35m:i32[]\u001b[39m = min m n\n          \u001b[34;1min \u001b[39;22m(k,) }\n      ] b -2147483648:i32[] 2147483647:i32[]\n      q\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] p\n      r\u001b[35m:i32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] o\n      s\u001b[35m:i32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] q\n      t\u001b[35m:key<fry>[2]\u001b[39m = random_split[shape=(2,)] a\n      u\u001b[35m:key<fry>[1]\u001b[39m = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] t\n      v\u001b[35m:key<fry>[]\u001b[39m = squeeze[dimensions=(0,)] u\n      w\u001b[35m:key<fry>[1]\u001b[39m = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] t\n      x\u001b[35m:key<fry>[]\u001b[39m = squeeze[dimensions=(0,)] w\n      y\u001b[35m:u32[1024]\u001b[39m = random_bits[bit_width=32 shape=(1024,)] v\n      z\u001b[35m:u32[1024]\u001b[39m = random_bits[bit_width=32 shape=(1024,)] x\n      ba\u001b[35m:i32[1]\u001b[39m = sub s r\n      bb\u001b[35m:u32[1]\u001b[39m = convert_element_type[new_dtype=uint32 weak_type=False] ba\n      bc\u001b[35m:bool[1]\u001b[39m = le s r\n      bd\u001b[35m:u32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] 1:u32[]\n      be\u001b[35m:u32[1]\u001b[39m = select_n bc bb bd\n      bf\u001b[35m:bool[1]\u001b[39m = gt s r\n      bg\u001b[35m:bool[1]\u001b[39m = and j bf\n      bh\u001b[35m:u32[1]\u001b[39m = add be 1:u32[]\n      bi\u001b[35m:u32[1]\u001b[39m = select_n bg be bh\n      bj\u001b[35m:u32[1]\u001b[39m = rem 65536:u32[] bi\n      bk\u001b[35m:u32[1]\u001b[39m = mul bj bj\n      bl\u001b[35m:u32[1]\u001b[39m = rem bk bi\n      bm\u001b[35m:u32[1024]\u001b[39m = rem y bi\n      bn\u001b[35m:u32[1024]\u001b[39m = mul bm bl\n      bo\u001b[35m:u32[1024]\u001b[39m = rem z bi\n      bp\u001b[35m:u32[1024]\u001b[39m = add bn bo\n      bq\u001b[35m:u32[1024]\u001b[39m = rem bp bi\n      br\u001b[35m:i32[1024]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] bq\n      c\u001b[35m:i32[1024]\u001b[39m = add r br\n    \u001b[34;1min \u001b[39;22m(c,) }\n] a 0:i32[] b\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:29:18 (setup_fixed_sample_selector.<locals>._sample_desired_samples)\n\n  operation b\u001b[35m:f32[1024,2]\u001b[39m = pjit[\n  name=clip\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[1024,2]\u001b[39m c\u001b[35m:f32[]\u001b[39m d\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22me\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] c\n      f\u001b[35m:f32[1024,2]\u001b[39m = max e a\n      g\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] d\n      b\u001b[35m:f32[1024,2]\u001b[39m = min g f\n    \u001b[34;1min \u001b[39;22m(b,) }\n] a -60.0:f32[] 60.0:f32[]\n    from line /tmp/ipykernel_3181388/301871262.py:14:18 (train_iter.<locals>.get_target_grads)\n\n  operation j\u001b[35m:f32[1024,2]\u001b[39m = pjit[\n  name=<lambda>\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[1024,1]\u001b[39m b\u001b[35m:f32[1,40,2]\u001b[39m c\u001b[35m:f32[1024,1,40,2]\u001b[39m d\u001b[35m:f32[1024,1,40]\u001b[39m\n      e\u001b[35m:f32[1024,1]\u001b[39m f\u001b[35m:bool[1024,1]\u001b[39m g\u001b[35m:f32[1]\u001b[39m h\u001b[35m:bool[1024,1]\u001b[39m i\u001b[35m:f32[1]\u001b[39m k\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22ml\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] k\n      m\u001b[35m:f32[1024]\u001b[39m = pjit[\n        name=_where\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; h\u001b[35m:bool[1024,1]\u001b[39m i\u001b[35m:f32[1]\u001b[39m l\u001b[35m:f32[1]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=()\n              shape=(1,)\n              sharding=None\n            ] 0.0:f32[]\n            o\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] l\n            p\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] n\n            q\u001b[35m:f32[1024,1]\u001b[39m = select_n h o p\n            m\u001b[35m:f32[1024]\u001b[39m = reduce_sum[axes=(np.int64(1),)] q\n          \u001b[34;1min \u001b[39;22m(m,) }\n      ] h i l\n      r\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0,)\n        shape=(1024, 1)\n        sharding=None\n      ] m\n      s\u001b[35m:f32[1024]\u001b[39m = pjit[\n        name=_where\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; f\u001b[35m:bool[1024,1]\u001b[39m g\u001b[35m:f32[1]\u001b[39m r\u001b[35m:f32[1024,1]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mt\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=()\n              shape=(1,)\n              sharding=None\n            ] 0.0:f32[]\n            u\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] t\n            v\u001b[35m:f32[1024,1]\u001b[39m = select_n f r u\n            s\u001b[35m:f32[1024]\u001b[39m = reduce_sum[axes=(np.int64(1),)] v\n          \u001b[34;1min \u001b[39;22m(s,) }\n      ] f g r\n      w\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0,)\n        shape=(1024, 1)\n        sharding=None\n      ] s\n      x\u001b[35m:f32[1024,1]\u001b[39m = div w e\n      y\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] 0.0:f32[]\n      z\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1),)\n        shape=(1024, 1)\n        sharding=None\n      ] y\n      ba\u001b[35m:f32[1024,1]\u001b[39m = select_n a x z\n      bb\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1),)\n        shape=(1024, 1)\n        sharding=None\n      ] y\n      bc\u001b[35m:f32[1024,1]\u001b[39m = select_n a bb x\n      bd\u001b[35m:f32[1024,1]\u001b[39m = neg ba\n      be\u001b[35m:f32[1024,1]\u001b[39m = add_any bc bd\n      bf\u001b[35m:f32[1024,1,40]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(1))\n        shape=(1024, 1, 40)\n        sharding=None\n      ] be\n      bg\u001b[35m:f32[1024,1,40]\u001b[39m = mul bf d\n      bh\u001b[35m:f32[1024,1,40,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(1), np.int64(2))\n        shape=(1024, 1, 40, 2)\n        sharding=None\n      ] bg\n      bi\u001b[35m:f32[1024,1,40,2]\u001b[39m = mul -0.5:f32[] bh\n      bj\u001b[35m:f32[1024,1,40,2]\u001b[39m = mul bi c\n      bk\u001b[35m:f32[1,1,40,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1), np.int64(2), np.int64(3))\n        shape=(1, 1, 40, 2)\n        sharding=None\n      ] b\n      bl\u001b[35m:f32[1024,1,40,2]\u001b[39m = div bj bk\n      bm\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1), np.int64(2))] bl\n      bn\u001b[35m:f32[1024,1,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(2))\n        shape=(1024, 1, 2)\n        sharding=None\n      ] bm\n      bo\u001b[35m:f32[1024,1,1,2]\u001b[39m = reshape[\n        dimensions=None\n        new_sizes=(1024, 1, 1, 2)\n        sharding=None\n      ] bn\n      bp\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1), np.int64(2))] bo\n      bq\u001b[35m:f32[1024,1,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(2))\n        shape=(1024, 1, 2)\n        sharding=None\n      ] bp\n      j\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1),)] bq\n    \u001b[34;1min \u001b[39;22m(j,) }\n] a b c d e f g h i 1.0:f32[]\n    from line /tmp/ipykernel_3181388/301871262.py:16:27 (train_iter.<locals>.get_target_grads)\n\n(Additional originating lines are not shown.)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConcretizationTypeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m key, subkey = jax.random.split(key)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# state = train_iter(state, subkey, target_log_prob)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m state = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_per_freq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m timer += time() - iter_time\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step % eval_freq == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (step == iterations - \u001b[32m1\u001b[39m):\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain_per_freq\u001b[39m\u001b[34m(state, key)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_freq):\n\u001b[32m     18\u001b[39m     key, subkey = jax.random.split(key)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     state = \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_log_prob\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_iter\u001b[39m\u001b[34m(train_state, key, target_log_prob_fn)\u001b[39m\n\u001b[32m     25\u001b[39m new_target_grads, new_target_lnpdfs = get_target_grads(new_samples)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# new_target_grads, new_target_lnpdfs = get_target_grads(low+ (high-low) * jax.nn.sigmoid(new_samples))\u001b[39;00m\n\u001b[32m     27\u001b[39m new_sample_db_state, samples, mapping, sample_dist_densities, target_lnpdfs, target_lnpdf_grads =\\\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mgmm_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_samples_and_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_db_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mnew_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mnew_target_lnpdfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mnew_target_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mnum_reused_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# print(\"total sample for trainiing samples\", samples.shape)\u001b[39;00m\n\u001b[32m     36\u001b[39m new_component_stepsizes = gmm_network.component_stepsize_fn(train_state.model_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:64\u001b[39m, in \u001b[36msetup_fixed_sample_selector.<locals>.save_samples_and_select\u001b[39m\u001b[34m(gmm_wrapper_state, sampledb_state, new_samples, new_target_lnpdfs, new_target_grads, mapping, num_reused_samples)\u001b[39m\n\u001b[32m     59\u001b[39m num_new_samples = TOTAL_SAMPLES - num_reused_samples\n\u001b[32m     60\u001b[39m sampledb_state = sample_db.add_samples(sampledb_state, new_samples, gmm_wrapper_state.gmm_state.means,\n\u001b[32m     61\u001b[39m                                        gmm_wrapper_state.gmm_state.chol_covs, new_target_lnpdfs,\n\u001b[32m     62\u001b[39m                                        new_target_grads, mapping)\n\u001b[32m     63\u001b[39m old_samples_pdf, samples, mapping, target_lnpdfs, target_grads = sample_db.get_newest_samples(\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     sampledb_state, \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_reused_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_new_samples\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sampledb_state, samples, mapping, old_samples_pdf, target_lnpdfs, target_grads\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mujoco/lib/python3.12/site-packages/jax/_src/core.py:1667\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[31mConcretizationTypeError\u001b[39m: Abstract tracer value encountered where concrete value is expected: traced array with shape float32[]\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function train_per_freq at /tmp/ipykernel_3181388/4095047525.py:16 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a\u001b[35m:f32[]\u001b[39m = floor 0.0:f32[]\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:48:31 (setup_fixed_sample_selector.<locals>.select_samples)\n\n  operation a\u001b[35m:f32[]\u001b[39m = min 0.0:f32[] b\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:49:29 (setup_fixed_sample_selector.<locals>.select_samples)\n\n  operation c\u001b[35m:i32[1024]\u001b[39m = pjit[\n  name=_randint\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:key<fry>[]\u001b[39m d\u001b[35m:i32[]\u001b[39m b\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22me\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; f\u001b[35m:i32[]\u001b[39m g\u001b[35m:i32[]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = max g f\n            e\u001b[35m:i32[]\u001b[39m = min h i\n          \u001b[34;1min \u001b[39;22m(e,) }\n      ] 2147483647:i32[] -2147483648:i32[] 2147483647:i32[]\n      j\u001b[35m:bool[]\u001b[39m = gt b e\n      k\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; d\u001b[35m:i32[]\u001b[39m l\u001b[35m:i32[]\u001b[39m m\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:i32[]\u001b[39m = max l d\n            k\u001b[35m:i32[]\u001b[39m = min m n\n          \u001b[34;1min \u001b[39;22m(k,) }\n      ] d -2147483648:i32[] 2147483647:i32[]\n      o\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] k\n      p\u001b[35m:i32[]\u001b[39m = pjit[\n        name=clip\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; d\u001b[35m:i32[]\u001b[39m l\u001b[35m:i32[]\u001b[39m m\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:i32[]\u001b[39m = max l d\n            k\u001b[35m:i32[]\u001b[39m = min m n\n          \u001b[34;1min \u001b[39;22m(k,) }\n      ] b -2147483648:i32[] 2147483647:i32[]\n      q\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] p\n      r\u001b[35m:i32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] o\n      s\u001b[35m:i32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] q\n      t\u001b[35m:key<fry>[2]\u001b[39m = random_split[shape=(2,)] a\n      u\u001b[35m:key<fry>[1]\u001b[39m = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] t\n      v\u001b[35m:key<fry>[]\u001b[39m = squeeze[dimensions=(0,)] u\n      w\u001b[35m:key<fry>[1]\u001b[39m = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] t\n      x\u001b[35m:key<fry>[]\u001b[39m = squeeze[dimensions=(0,)] w\n      y\u001b[35m:u32[1024]\u001b[39m = random_bits[bit_width=32 shape=(1024,)] v\n      z\u001b[35m:u32[1024]\u001b[39m = random_bits[bit_width=32 shape=(1024,)] x\n      ba\u001b[35m:i32[1]\u001b[39m = sub s r\n      bb\u001b[35m:u32[1]\u001b[39m = convert_element_type[new_dtype=uint32 weak_type=False] ba\n      bc\u001b[35m:bool[1]\u001b[39m = le s r\n      bd\u001b[35m:u32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] 1:u32[]\n      be\u001b[35m:u32[1]\u001b[39m = select_n bc bb bd\n      bf\u001b[35m:bool[1]\u001b[39m = gt s r\n      bg\u001b[35m:bool[1]\u001b[39m = and j bf\n      bh\u001b[35m:u32[1]\u001b[39m = add be 1:u32[]\n      bi\u001b[35m:u32[1]\u001b[39m = select_n bg be bh\n      bj\u001b[35m:u32[1]\u001b[39m = rem 65536:u32[] bi\n      bk\u001b[35m:u32[1]\u001b[39m = mul bj bj\n      bl\u001b[35m:u32[1]\u001b[39m = rem bk bi\n      bm\u001b[35m:u32[1024]\u001b[39m = rem y bi\n      bn\u001b[35m:u32[1024]\u001b[39m = mul bm bl\n      bo\u001b[35m:u32[1024]\u001b[39m = rem z bi\n      bp\u001b[35m:u32[1024]\u001b[39m = add bn bo\n      bq\u001b[35m:u32[1024]\u001b[39m = rem bp bi\n      br\u001b[35m:i32[1024]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] bq\n      c\u001b[35m:i32[1024]\u001b[39m = add r br\n    \u001b[34;1min \u001b[39;22m(c,) }\n] a 0:i32[] b\n    from line /home/sukchul/distributionally_robust_learning/learning/module/gmmvi/sample_selector.py:29:18 (setup_fixed_sample_selector.<locals>._sample_desired_samples)\n\n  operation b\u001b[35m:f32[1024,2]\u001b[39m = pjit[\n  name=clip\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[1024,2]\u001b[39m c\u001b[35m:f32[]\u001b[39m d\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22me\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] c\n      f\u001b[35m:f32[1024,2]\u001b[39m = max e a\n      g\u001b[35m:f32[]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] d\n      b\u001b[35m:f32[1024,2]\u001b[39m = min g f\n    \u001b[34;1min \u001b[39;22m(b,) }\n] a -60.0:f32[] 60.0:f32[]\n    from line /tmp/ipykernel_3181388/301871262.py:14:18 (train_iter.<locals>.get_target_grads)\n\n  operation j\u001b[35m:f32[1024,2]\u001b[39m = pjit[\n  name=<lambda>\n  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[1024,1]\u001b[39m b\u001b[35m:f32[1,40,2]\u001b[39m c\u001b[35m:f32[1024,1,40,2]\u001b[39m d\u001b[35m:f32[1024,1,40]\u001b[39m\n      e\u001b[35m:f32[1024,1]\u001b[39m f\u001b[35m:bool[1024,1]\u001b[39m g\u001b[35m:f32[1]\u001b[39m h\u001b[35m:bool[1024,1]\u001b[39m i\u001b[35m:f32[1]\u001b[39m k\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n      \u001b[39;22ml\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] k\n      m\u001b[35m:f32[1024]\u001b[39m = pjit[\n        name=_where\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; h\u001b[35m:bool[1024,1]\u001b[39m i\u001b[35m:f32[1]\u001b[39m l\u001b[35m:f32[1]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mn\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=()\n              shape=(1,)\n              sharding=None\n            ] 0.0:f32[]\n            o\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] l\n            p\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] n\n            q\u001b[35m:f32[1024,1]\u001b[39m = select_n h o p\n            m\u001b[35m:f32[1024]\u001b[39m = reduce_sum[axes=(np.int64(1),)] q\n          \u001b[34;1min \u001b[39;22m(m,) }\n      ] h i l\n      r\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0,)\n        shape=(1024, 1)\n        sharding=None\n      ] m\n      s\u001b[35m:f32[1024]\u001b[39m = pjit[\n        name=_where\n        jaxpr={ \u001b[34;1mlambda \u001b[39;22m; f\u001b[35m:bool[1024,1]\u001b[39m g\u001b[35m:f32[1]\u001b[39m r\u001b[35m:f32[1024,1]\u001b[39m. \u001b[34;1mlet\n            \u001b[39;22mt\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=()\n              shape=(1,)\n              sharding=None\n            ] 0.0:f32[]\n            u\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n              broadcast_dimensions=(np.int64(1),)\n              shape=(1024, 1)\n              sharding=None\n            ] t\n            v\u001b[35m:f32[1024,1]\u001b[39m = select_n f r u\n            s\u001b[35m:f32[1024]\u001b[39m = reduce_sum[axes=(np.int64(1),)] v\n          \u001b[34;1min \u001b[39;22m(s,) }\n      ] f g r\n      w\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0,)\n        shape=(1024, 1)\n        sharding=None\n      ] s\n      x\u001b[35m:f32[1024,1]\u001b[39m = div w e\n      y\u001b[35m:f32[1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=()\n        shape=(1,)\n        sharding=None\n      ] 0.0:f32[]\n      z\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1),)\n        shape=(1024, 1)\n        sharding=None\n      ] y\n      ba\u001b[35m:f32[1024,1]\u001b[39m = select_n a x z\n      bb\u001b[35m:f32[1024,1]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1),)\n        shape=(1024, 1)\n        sharding=None\n      ] y\n      bc\u001b[35m:f32[1024,1]\u001b[39m = select_n a bb x\n      bd\u001b[35m:f32[1024,1]\u001b[39m = neg ba\n      be\u001b[35m:f32[1024,1]\u001b[39m = add_any bc bd\n      bf\u001b[35m:f32[1024,1,40]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(1))\n        shape=(1024, 1, 40)\n        sharding=None\n      ] be\n      bg\u001b[35m:f32[1024,1,40]\u001b[39m = mul bf d\n      bh\u001b[35m:f32[1024,1,40,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(1), np.int64(2))\n        shape=(1024, 1, 40, 2)\n        sharding=None\n      ] bg\n      bi\u001b[35m:f32[1024,1,40,2]\u001b[39m = mul -0.5:f32[] bh\n      bj\u001b[35m:f32[1024,1,40,2]\u001b[39m = mul bi c\n      bk\u001b[35m:f32[1,1,40,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(np.int64(1), np.int64(2), np.int64(3))\n        shape=(1, 1, 40, 2)\n        sharding=None\n      ] b\n      bl\u001b[35m:f32[1024,1,40,2]\u001b[39m = div bj bk\n      bm\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1), np.int64(2))] bl\n      bn\u001b[35m:f32[1024,1,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(2))\n        shape=(1024, 1, 2)\n        sharding=None\n      ] bm\n      bo\u001b[35m:f32[1024,1,1,2]\u001b[39m = reshape[\n        dimensions=None\n        new_sizes=(1024, 1, 1, 2)\n        sharding=None\n      ] bn\n      bp\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1), np.int64(2))] bo\n      bq\u001b[35m:f32[1024,1,2]\u001b[39m = broadcast_in_dim[\n        broadcast_dimensions=(0, np.int64(2))\n        shape=(1024, 1, 2)\n        sharding=None\n      ] bp\n      j\u001b[35m:f32[1024,2]\u001b[39m = reduce_sum[axes=(np.int64(1),)] bq\n    \u001b[34;1min \u001b[39;22m(j,) }\n] a b c d e f g h i 1.0:f32[]\n    from line /tmp/ipykernel_3181388/301871262.py:16:27 (train_iter.<locals>.get_target_grads)\n\n(Additional originating lines are not shown.)\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "iterations = 3000\n",
    "seed=0\n",
    "num_evals = 100\n",
    "eval_freq = iterations//num_evals\n",
    "n_eval_samples= 1000\n",
    "target_samples = target.sample(jax.random.PRNGKey(0), (n_eval_samples,))\n",
    "target._plot_bound\n",
    "low = jnp.array([-target._plot_bound, -target._plot_bound])\n",
    "high = jnp.array([target._plot_bound, target._plot_bound])\n",
    "target_log_prob = jax.jit(lambda x : target.log_prob(x))\n",
    "rng =jax.random.PRNGKey(seed)\n",
    "key, rng = jax.random.split(rng)\n",
    "timer = 0\n",
    "state = initial_train_state\n",
    "\n",
    "def train_per_freq(state, key):\n",
    "    for i in range(eval_freq):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        state = train_iter(state, subkey, target_log_prob)\n",
    "\n",
    "for step in range(0, iterations, eval_freq):\n",
    "    iter_time = time()\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # state = train_iter(state, subkey, target_log_prob)\n",
    "    state = jax.jit(train_per_freq)(state, subkey)\n",
    "    timer += time() - iter_time\n",
    "    if (step % eval_freq == 0) or (step == iterations - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        logger = eval_fn(*eval(subkey, state, target_log_prob, n_eval_samples, target_samples))\n",
    "        logger[\"stats/step\"].append(step)\n",
    "        logger[\"stats/wallclock\"].append(timer)\n",
    "        logger['stats/num_samples'] = [state.sample_db_state.num_samples_written]\n",
    "        logger['stats/num_components'] = [state.model_state.gmm_state.num_components]\n",
    "        print(f\"{step}/{iterations}: \"\n",
    "                f\"The model now has {state.model_state.gmm_state.num_components} components \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
