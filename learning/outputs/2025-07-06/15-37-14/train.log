[2025-07-06 15:37:16,177][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 15:37:16,366][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:37:16,366][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:37:16,854][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 15:37:17,545][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:37:17,546][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:37:50,678][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 15:37:50,678][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 15:39:15,664][absl][INFO] - {'eval/walltime': 84.86439990997314, 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(190.315, dtype=float32), 'eval/episode_reward/small_control': Array(891.627, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(613.19, dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(10.359, dtype=float32), 'eval/episode_reward/small_control_std': Array(49.075, dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(76.547, dtype=float32), 'eval/avg_episode_length': Array(995.148, dtype=float32), 'eval/std_episode_length': Array(54.674, dtype=float32), 'eval/epoch_eval_time': 84.86439990997314, 'eval/sps': 1508.2885183396863}
[2025-07-06 15:39:15,687][absl][INFO] - starting iteration 0 118.83239912986755
[2025-07-06 15:44:10,062][absl][INFO] - {'eval/walltime': 100.81496715545654, 'training/sps': np.float64(24764.57832089712), 'training/walltime': 277.867844581604, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.950567245483398, 'eval/sps': 8024.792976327835}
[2025-07-06 15:44:10,078][absl][INFO] - starting iteration 1 413.22360610961914
