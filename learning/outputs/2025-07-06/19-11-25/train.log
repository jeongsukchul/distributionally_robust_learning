[2025-07-06 19:11:27,481][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 19:11:27,858][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:11:27,859][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:11:28,690][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 19:11:29,480][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:11:29,480][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:12:02,508][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:12:02,508][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:13:28,169][absl][INFO] - {'eval/walltime': 85.54149174690247, 'eval/episode_reward': Array(1.725, dtype=float32), 'eval/episode_reward/move': Array(191.252, dtype=float32), 'eval/episode_reward/small_control': Array(895.802, dtype=float32), 'eval/episode_reward/stand': Array(11.075, dtype=float32), 'eval/episode_reward/standing': Array(11.123, dtype=float32), 'eval/episode_reward/upright': Array(615.075, dtype=float32), 'eval/episode_reward_std': Array(0.154, dtype=float32), 'eval/episode_reward/move_std': Array(1.896, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.332, dtype=float32), 'eval/episode_reward/stand_std': Array(0.688, dtype=float32), 'eval/episode_reward/standing_std': Array(0.691, dtype=float32), 'eval/episode_reward/upright_std': Array(62.345, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 85.54149174690247, 'eval/sps': 1496.34986935606}
[2025-07-06 19:13:28,193][absl][INFO] - starting iteration 0 119.5031189918518
[2025-07-06 19:18:21,916][absl][INFO] - {'eval/walltime': 101.67962718009949, 'training/sps': np.float64(24825.15321140024), 'training/walltime': 277.18983006477356, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.13813543319702, 'eval/sps': 7931.523473070938}
[2025-07-06 19:18:21,937][absl][INFO] - starting iteration 1 413.2469937801361
