[2025-07-06 19:21:59,232][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 19:21:59,462][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:21:59,462][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:21:59,947][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 19:22:00,656][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:22:00,657][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:22:33,351][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 19:22:33,351][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 19:23:57,254][absl][INFO] - {'eval/walltime': 83.78648161888123, 'eval/episode_reward': Array(1.725, dtype=float32), 'eval/episode_reward/move': Array(191.577, dtype=float32), 'eval/episode_reward/small_control': Array(895.775, dtype=float32), 'eval/episode_reward/stand': Array(11.073, dtype=float32), 'eval/episode_reward/standing': Array(11.121, dtype=float32), 'eval/episode_reward/upright': Array(615.067, dtype=float32), 'eval/episode_reward_std': Array(0.15, dtype=float32), 'eval/episode_reward/move_std': Array(1.84, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.368, dtype=float32), 'eval/episode_reward/stand_std': Array(0.659, dtype=float32), 'eval/episode_reward/standing_std': Array(0.65, dtype=float32), 'eval/episode_reward/upright_std': Array(54.849, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 83.78648161888123, 'eval/sps': 1527.6927438274874}
[2025-07-06 19:23:57,276][absl][INFO] - starting iteration 0 117.32876396179199
[2025-07-06 19:28:48,968][absl][INFO] - {'eval/walltime': 99.76759362220764, 'training/sps': np.float64(24992.883588084536), 'training/walltime': 275.32957434654236, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.981112003326416, 'eval/sps': 8009.455160151385}
[2025-07-06 19:28:48,983][absl][INFO] - starting iteration 1 409.03577423095703
[2025-07-06 19:32:32,978][absl][INFO] - {'eval/walltime': 114.63405346870422, 'training/sps': np.float64(32906.62464161296), 'training/walltime': 484.4449143409729, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 14.866459846496582, 'eval/sps': 8609.985250130978}
[2025-07-06 19:32:32,992][absl][INFO] - starting iteration 2 633.0450007915497
[2025-07-06 19:36:19,714][absl][INFO] - {'eval/walltime': 131.4680631160736, 'training/sps': np.float64(32788.01515706566), 'training/walltime': 694.3167216777802, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.834009647369385, 'eval/sps': 7603.654903453277}
[2025-07-06 19:36:19,727][absl][INFO] - starting iteration 3 859.7804064750671
[2025-07-06 19:39:58,332][absl][INFO] - {'eval/walltime': 146.93613624572754, 'training/sps': np.float64(33878.03048399204), 'training/walltime': 897.4359674453735, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.46807312965393, 'eval/sps': 8275.109570991779}
[2025-07-06 19:39:58,345][absl][INFO] - starting iteration 4 1078.3978435993195
[2025-07-06 19:43:36,245][absl][INFO] - {'eval/walltime': 162.95023846626282, 'training/sps': np.float64(34087.2910680066), 'training/walltime': 1099.3082718849182, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.01410222053528, 'eval/sps': 7992.955099029057}
[2025-07-06 19:43:36,256][absl][INFO] - starting iteration 5 1296.3096783161163
[2025-07-06 19:47:13,410][absl][INFO] - {'eval/walltime': 178.0608274936676, 'training/sps': np.float64(34060.97037670691), 'training/walltime': 1301.336573600769, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.110589027404785, 'eval/sps': 8470.880901324053}
[2025-07-06 19:47:13,425][absl][INFO] - starting iteration 6 1513.4786968231201
[2025-07-06 19:50:51,319][absl][INFO] - {'eval/walltime': 194.1514778137207, 'training/sps': np.float64(34101.313458815275), 'training/walltime': 1503.1258685588837, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.0906503200531, 'eval/sps': 7954.930189519996}
[2025-07-06 19:50:51,335][absl][INFO] - starting iteration 7 1731.3886377811432
[2025-07-06 19:54:28,841][absl][INFO] - {'eval/walltime': 209.7895667552948, 'training/sps': np.float64(34090.70314264239), 'training/walltime': 1704.9779679775238, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.638088941574097, 'eval/sps': 8185.143368746935}
[2025-07-06 19:54:28,854][absl][INFO] - starting iteration 8 1948.907201051712
[2025-07-06 19:58:06,175][absl][INFO] - {'eval/walltime': 225.3601109981537, 'training/sps': np.float64(34110.27050879351), 'training/walltime': 1906.7142748832703, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 15.570544242858887, 'eval/sps': 8220.650351300636}
