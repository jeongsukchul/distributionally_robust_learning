[2025-07-06 23:04:29,256][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-06 23:04:29,461][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 23:04:29,462][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 23:04:29,852][absl][INFO] - Device count: 1, process count: 1 (id 0), local device count: 1, devices to be used count: 1
[2025-07-06 23:04:30,577][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 23:04:30,577][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 23:05:06,229][root][INFO] - Using JAX default device: cuda:0.
[2025-07-06 23:05:06,230][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-06 23:06:37,730][absl][INFO] - {'eval/walltime': 91.37902975082397, 'eval/episode_reward': Array(1.726, dtype=float32), 'eval/episode_reward/move': Array(191.399, dtype=float32), 'eval/episode_reward/small_control': Array(895.745, dtype=float32), 'eval/episode_reward/stand': Array(11.08, dtype=float32), 'eval/episode_reward/standing': Array(11.129, dtype=float32), 'eval/episode_reward/upright': Array(607.909, dtype=float32), 'eval/episode_reward_std': Array(0.154, dtype=float32), 'eval/episode_reward/move_std': Array(1.906, dtype=float32), 'eval/episode_reward/small_control_std': Array(1.487, dtype=float32), 'eval/episode_reward/stand_std': Array(0.695, dtype=float32), 'eval/episode_reward/standing_std': Array(0.699, dtype=float32), 'eval/episode_reward/upright_std': Array(59.026, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 91.37902975082397, 'eval/sps': 1400.7590182237168}
[2025-07-06 23:06:37,752][absl][INFO] - starting iteration 0 127.89973449707031
[2025-07-06 23:11:48,123][absl][INFO] - {'eval/walltime': 107.89715766906738, 'training/sps': np.float64(23450.477951645604), 'training/walltime': 293.43879532814026, 'training/entropy_loss': Array(nan, dtype=float32), 'training/policy_loss': Array(nan, dtype=float32), 'training/total_loss': Array(nan, dtype=float32), 'training/v_loss': Array(nan, dtype=float32), 'eval/episode_reward': Array(nan, dtype=float32), 'eval/episode_reward/move': Array(0.167, dtype=float32), 'eval/episode_reward/small_control': Array(0.8, dtype=float32), 'eval/episode_reward/stand': Array(nan, dtype=float32), 'eval/episode_reward/standing': Array(nan, dtype=float32), 'eval/episode_reward/upright': Array(0., dtype=float32), 'eval/episode_reward_std': Array(nan, dtype=float32), 'eval/episode_reward/move_std': Array(0., dtype=float32), 'eval/episode_reward/small_control_std': Array(0., dtype=float32), 'eval/episode_reward/stand_std': Array(nan, dtype=float32), 'eval/episode_reward/standing_std': Array(nan, dtype=float32), 'eval/episode_reward/upright_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 16.518127918243408, 'eval/sps': 7749.062159679166}
[2025-07-06 23:11:48,140][absl][INFO] - starting iteration 1 438.2880599498749
