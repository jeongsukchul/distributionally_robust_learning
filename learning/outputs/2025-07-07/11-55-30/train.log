[2025-07-07 11:55:32,453][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 11:55:32,604][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 11:55:32,605][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 11:55:32,827][absl][INFO] - local_device_count: 1; total_device_count: 1
[2025-07-07 11:55:33,169][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 11:55:33,169][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 11:55:38,772][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 11:55:38,772][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 11:55:57,281][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 11:55:57,281][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 11:56:39,861][absl][INFO] - {'eval/walltime': 42.42040419578552, 'eval/episode_reward': Array(0.049, dtype=float32), 'eval/episode_reward/hopping': Array(43.843, dtype=float32), 'eval/episode_reward/standing': Array(1.344, dtype=float32), 'eval/episode_reward_std': Array(0.308, dtype=float32), 'eval/episode_reward/hopping_std': Array(13.87, dtype=float32), 'eval/episode_reward/standing_std': Array(3.817, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 42.42040419578552, 'eval/sps': 3017.415850382605}
[2025-07-07 11:57:01,954][absl][INFO] - replay size after prefill 8192
[2025-07-07 11:57:02,010][absl][INFO] - step 0
[2025-07-07 11:58:25,606][absl][INFO] - {'eval/walltime': 45.81530165672302, 'training/sps': 13846.584792250515, 'training/walltime': 102.32753300666809, 'training/actor_loss': Array(-15.335, dtype=float32), 'training/alpha': Array(0.059, dtype=float32), 'training/alpha_loss': Array(0.276, dtype=float32), 'training/buffer_current_size': Array(563392.06, dtype=float32), 'training/critic_loss': Array(0.004, dtype=float32), 'eval/episode_reward': Array(39.245, dtype=float32), 'eval/episode_reward/hopping': Array(87.275, dtype=float32), 'eval/episode_reward/standing': Array(174.641, dtype=float32), 'eval/episode_reward_std': Array(49.87, dtype=float32), 'eval/episode_reward/hopping_std': Array(100.484, dtype=float32), 'eval/episode_reward/standing_std': Array(218.231, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.3948974609375, 'eval/sps': 37703.64244363741}
[2025-07-07 11:58:25,611][absl][INFO] - step 1118464
[2025-07-07 11:59:19,241][absl][INFO] - {'eval/walltime': 49.20116949081421, 'training/sps': 22100.1365302043, 'training/walltime': 152.56577467918396, 'training/actor_loss': Array(-3.975, dtype=float32), 'training/alpha': Array(0.001, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(1.674e+06, dtype=float32), 'training/critic_loss': Array(0.002, dtype=float32), 'eval/episode_reward': Array(146.218, dtype=float32), 'eval/episode_reward/hopping': Array(268.684, dtype=float32), 'eval/episode_reward/standing': Array(493.266, dtype=float32), 'eval/episode_reward_std': Array(35.658, dtype=float32), 'eval/episode_reward/hopping_std': Array(62.351, dtype=float32), 'eval/episode_reward/standing_std': Array(120.518, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.3858678340911865, 'eval/sps': 37804.19268324954}
[2025-07-07 11:59:19,246][absl][INFO] - step 2228736
[2025-07-07 12:00:13,085][absl][INFO] - {'eval/walltime': 52.60953450202942, 'training/sps': 22018.248581106807, 'training/walltime': 202.9908571243286, 'training/actor_loss': Array(-10.055, dtype=float32), 'training/alpha': Array(0.006, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(2.784e+06, dtype=float32), 'training/critic_loss': Array(0.005, dtype=float32), 'eval/episode_reward': Array(198.621, dtype=float32), 'eval/episode_reward/hopping': Array(356.303, dtype=float32), 'eval/episode_reward/standing': Array(513.844, dtype=float32), 'eval/episode_reward_std': Array(18.141, dtype=float32), 'eval/episode_reward/hopping_std': Array(31.1, dtype=float32), 'eval/episode_reward/standing_std': Array(47.215, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.40836501121521, 'eval/sps': 37554.66318273324}
[2025-07-07 12:00:13,090][absl][INFO] - step 3339008
[2025-07-07 12:01:12,907][absl][INFO] - {'eval/walltime': 56.03728270530701, 'training/sps': 19691.383724387677, 'training/walltime': 259.374502658844, 'training/actor_loss': Array(-15.344, dtype=float32), 'training/alpha': Array(0.008, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(3.865e+06, dtype=float32), 'training/critic_loss': Array(0.006, dtype=float32), 'eval/episode_reward': Array(260.122, dtype=float32), 'eval/episode_reward/hopping': Array(449.59, dtype=float32), 'eval/episode_reward/standing': Array(514.164, dtype=float32), 'eval/episode_reward_std': Array(4.715, dtype=float32), 'eval/episode_reward/hopping_std': Array(8.063, dtype=float32), 'eval/episode_reward/standing_std': Array(10.325, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.427748203277588, 'eval/sps': 37342.299494930034}
[2025-07-07 12:01:12,912][absl][INFO] - step 4449280
[2025-07-07 12:02:32,901][absl][INFO] - {'eval/walltime': 59.462470293045044, 'training/sps': 14502.275187108527, 'training/walltime': 335.93297266960144, 'training/actor_loss': Array(-19.787, dtype=float32), 'training/alpha': Array(0.011, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.006, dtype=float32), 'eval/episode_reward': Array(273.043, dtype=float32), 'eval/episode_reward/hopping': Array(497.531, dtype=float32), 'eval/episode_reward/standing': Array(489.016, dtype=float32), 'eval/episode_reward_std': Array(5.594, dtype=float32), 'eval/episode_reward/hopping_std': Array(7.704, dtype=float32), 'eval/episode_reward/standing_std': Array(15.308, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.425187587738037, 'eval/sps': 37370.216001667235}
[2025-07-07 12:02:32,906][absl][INFO] - step 5559552
[2025-07-07 12:03:53,031][absl][INFO] - {'eval/walltime': 62.889599561691284, 'training/sps': 14477.007357323775, 'training/walltime': 412.6250660419464, 'training/actor_loss': Array(-22.859, dtype=float32), 'training/alpha': Array(0.013, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.005, dtype=float32), 'eval/episode_reward': Array(286.502, dtype=float32), 'eval/episode_reward/hopping': Array(512.432, dtype=float32), 'eval/episode_reward/standing': Array(499.867, dtype=float32), 'eval/episode_reward_std': Array(3.046, dtype=float32), 'eval/episode_reward/hopping_std': Array(6.346, dtype=float32), 'eval/episode_reward/standing_std': Array(11.59, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.4271292686462402, 'eval/sps': 37349.043460669236}
[2025-07-07 12:03:53,036][absl][INFO] - step 6669824
[2025-07-07 12:05:13,205][absl][INFO] - {'eval/walltime': 66.3238217830658, 'training/sps': 14469.911355324604, 'training/walltime': 489.35476899147034, 'training/actor_loss': Array(-24.735, dtype=float32), 'training/alpha': Array(0.014, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.004, dtype=float32), 'eval/episode_reward': Array(294.41, dtype=float32), 'eval/episode_reward/hopping': Array(522.18, dtype=float32), 'eval/episode_reward/standing': Array(503.656, dtype=float32), 'eval/episode_reward_std': Array(3.441, dtype=float32), 'eval/episode_reward/hopping_std': Array(7.416, dtype=float32), 'eval/episode_reward/standing_std': Array(12.46, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.4342222213745117, 'eval/sps': 37271.90372344901}
[2025-07-07 12:05:13,210][absl][INFO] - step 7780096
[2025-07-07 12:06:33,558][absl][INFO] - {'eval/walltime': 69.76704168319702, 'training/sps': 14437.83600844289, 'training/walltime': 566.2549359798431, 'training/actor_loss': Array(-26.074, dtype=float32), 'training/alpha': Array(0.014, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.004, dtype=float32), 'eval/episode_reward': Array(298.763, dtype=float32), 'eval/episode_reward/hopping': Array(545.276, dtype=float32), 'eval/episode_reward/standing': Array(496.984, dtype=float32), 'eval/episode_reward_std': Array(4.319, dtype=float32), 'eval/episode_reward/hopping_std': Array(8.104, dtype=float32), 'eval/episode_reward/standing_std': Array(13.376, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.4432199001312256, 'eval/sps': 37174.50633783853}
[2025-07-07 12:06:33,563][absl][INFO] - step 8890368
[2025-07-07 12:07:53,869][absl][INFO] - {'eval/walltime': 73.19742918014526, 'training/sps': 14443.442591590298, 'training/walltime': 643.1252522468567, 'training/actor_loss': Array(-26.836, dtype=float32), 'training/alpha': Array(0.014, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.004, dtype=float32), 'eval/episode_reward': Array(292.598, dtype=float32), 'eval/episode_reward/hopping': Array(526.049, dtype=float32), 'eval/episode_reward/standing': Array(488.859, dtype=float32), 'eval/episode_reward_std': Array(52.729, dtype=float32), 'eval/episode_reward/hopping_std': Array(94.294, dtype=float32), 'eval/episode_reward/standing_std': Array(88.712, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.430387496948242, 'eval/sps': 37313.56883555341}
[2025-07-07 12:07:55,074][absl][INFO] - total steps: 10000640
