[2025-07-07 15:46:38,853][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-07-07 15:46:39,011][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:39,011][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:39,250][absl][INFO] - local_device_count: 1; total_device_count: 1
[2025-07-07 15:46:39,641][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:39,641][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:45,501][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:45,501][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:46:59,036][root][INFO] - Using JAX default device: cuda:0.
[2025-07-07 15:46:59,036][root][INFO] - MJX Warp is disabled via MJX_WARP_ENABLED=false.
[2025-07-07 15:47:31,713][absl][INFO] - {'eval/walltime': 32.53706765174866, 'eval/episode_reward': Array(68.493, dtype=float32), 'eval/episode_reward/in_target': Array(4.573, dtype=float32), 'eval/episode_reward/upright': Array(515.933, dtype=float32), 'eval/episode_reward_std': Array(35.917, dtype=float32), 'eval/episode_reward/in_target_std': Array(26.554, dtype=float32), 'eval/episode_reward/upright_std': Array(243.381, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 32.53706765174866, 'eval/sps': 3933.974670674443}
[2025-07-07 15:47:48,728][absl][INFO] - replay size after prefill 8192
[2025-07-07 15:47:48,763][absl][INFO] - step 0
[2025-07-07 15:48:46,429][absl][INFO] - {'eval/walltime': 36.387571573257446, 'training/sps': 10312.454779861704, 'training/walltime': 70.83995842933655, 'training/actor_loss': Array(-33.679, dtype=float32), 'training/alpha': Array(0.118, dtype=float32), 'training/alpha_loss': Array(0.688, dtype=float32), 'training/buffer_current_size': Array(285632., dtype=float32), 'training/critic_loss': Array(0.031, dtype=float32), 'eval/episode_reward': Array(85.885, dtype=float32), 'eval/episode_reward/in_target': Array(21.429, dtype=float32), 'eval/episode_reward/upright': Array(537.078, dtype=float32), 'eval/episode_reward_std': Array(94.976, dtype=float32), 'eval/episode_reward/in_target_std': Array(99.916, dtype=float32), 'eval/episode_reward/upright_std': Array(253.332, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.850503921508789, 'eval/sps': 33242.40219182642}
[2025-07-07 15:48:46,435][absl][INFO] - step 562944
[2025-07-07 15:49:17,910][absl][INFO] - {'eval/walltime': 40.13134455680847, 'training/sps': 20008.943943456623, 'training/walltime': 98.56515979766846, 'training/actor_loss': Array(-16.599, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(840383.94, dtype=float32), 'training/critic_loss': Array(0.063, dtype=float32), 'eval/episode_reward': Array(120.725, dtype=float32), 'eval/episode_reward/in_target': Array(61.628, dtype=float32), 'eval/episode_reward/upright': Array(534.404, dtype=float32), 'eval/episode_reward_std': Array(165.162, dtype=float32), 'eval/episode_reward/in_target_std': Array(186.418, dtype=float32), 'eval/episode_reward/upright_std': Array(234.645, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.7437729835510254, 'eval/sps': 34190.10729614007}
[2025-07-07 15:49:17,915][absl][INFO] - step 1117696
[2025-07-07 15:49:49,614][absl][INFO] - {'eval/walltime': 44.04436373710632, 'training/sps': 19970.219675284265, 'training/walltime': 126.3441231250763, 'training/actor_loss': Array(-14.889, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(1.395e+06, dtype=float32), 'training/critic_loss': Array(0.05, dtype=float32), 'eval/episode_reward': Array(178.517, dtype=float32), 'eval/episode_reward/in_target': Array(124.249, dtype=float32), 'eval/episode_reward/upright': Array(558.396, dtype=float32), 'eval/episode_reward_std': Array(232.169, dtype=float32), 'eval/episode_reward/in_target_std': Array(262.845, dtype=float32), 'eval/episode_reward/upright_std': Array(231.332, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.9130191802978516, 'eval/sps': 32711.31423134422}
[2025-07-07 15:49:49,625][absl][INFO] - step 1672448
[2025-07-07 15:50:21,252][absl][INFO] - {'eval/walltime': 47.948872327804565, 'training/sps': 20015.74501197909, 'training/walltime': 154.05990386009216, 'training/actor_loss': Array(-15.928, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(1.95e+06, dtype=float32), 'training/critic_loss': Array(0.04, dtype=float32), 'eval/episode_reward': Array(187.046, dtype=float32), 'eval/episode_reward/in_target': Array(137.441, dtype=float32), 'eval/episode_reward/upright': Array(534.282, dtype=float32), 'eval/episode_reward_std': Array(252.693, dtype=float32), 'eval/episode_reward/in_target_std': Array(286.332, dtype=float32), 'eval/episode_reward/upright_std': Array(257.344, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.904508590698242, 'eval/sps': 32782.61451516228}
[2025-07-07 15:50:21,259][absl][INFO] - step 2227200
[2025-07-07 15:50:53,233][absl][INFO] - {'eval/walltime': 51.798868894577026, 'training/sps': 19729.90141884265, 'training/walltime': 182.17722630500793, 'training/actor_loss': Array(-17.349, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(2.505e+06, dtype=float32), 'training/critic_loss': Array(0.032, dtype=float32), 'eval/episode_reward': Array(238.716, dtype=float32), 'eval/episode_reward/in_target': Array(189.477, dtype=float32), 'eval/episode_reward/upright': Array(583.388, dtype=float32), 'eval/episode_reward_std': Array(280.128, dtype=float32), 'eval/episode_reward/in_target_std': Array(318.363, dtype=float32), 'eval/episode_reward/upright_std': Array(234.323, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.849996566772461, 'eval/sps': 33246.78289448588}
[2025-07-07 15:50:53,239][absl][INFO] - step 2781952
[2025-07-07 15:51:24,941][absl][INFO] - {'eval/walltime': 55.661505460739136, 'training/sps': 19930.9089876584, 'training/walltime': 210.0109794139862, 'training/actor_loss': Array(-19.055, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(3.059e+06, dtype=float32), 'training/critic_loss': Array(0.029, dtype=float32), 'eval/episode_reward': Array(273.595, dtype=float32), 'eval/episode_reward/in_target': Array(232.899, dtype=float32), 'eval/episode_reward/upright': Array(558.47, dtype=float32), 'eval/episode_reward_std': Array(291.44, dtype=float32), 'eval/episode_reward/in_target_std': Array(330.141, dtype=float32), 'eval/episode_reward/upright_std': Array(214.664, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8626365661621094, 'eval/sps': 33137.98691839651}
[2025-07-07 15:51:24,946][absl][INFO] - step 3336704
[2025-07-07 15:51:56,639][absl][INFO] - {'eval/walltime': 59.46162557601929, 'training/sps': 19895.648613414873, 'training/walltime': 237.8940613269806, 'training/actor_loss': Array(-20.692, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(3.614e+06, dtype=float32), 'training/critic_loss': Array(0.027, dtype=float32), 'eval/episode_reward': Array(279.193, dtype=float32), 'eval/episode_reward/in_target': Array(229.234, dtype=float32), 'eval/episode_reward/upright': Array(628.91, dtype=float32), 'eval/episode_reward_std': Array(304.618, dtype=float32), 'eval/episode_reward/in_target_std': Array(345.042, dtype=float32), 'eval/episode_reward/upright_std': Array(210.988, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8001201152801514, 'eval/sps': 33683.145826184926}
[2025-07-07 15:51:56,645][absl][INFO] - step 3891456
[2025-07-07 15:52:37,953][absl][INFO] - {'eval/walltime': 63.26366448402405, 'training/sps': 14793.599607578986, 'training/walltime': 275.3935215473175, 'training/actor_loss': Array(-22.297, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(0., dtype=float32), 'training/buffer_current_size': Array(4.112e+06, dtype=float32), 'training/critic_loss': Array(0.025, dtype=float32), 'eval/episode_reward': Array(280.029, dtype=float32), 'eval/episode_reward/in_target': Array(240.43, dtype=float32), 'eval/episode_reward/upright': Array(557.22, dtype=float32), 'eval/episode_reward_std': Array(302.608, dtype=float32), 'eval/episode_reward/in_target_std': Array(348.445, dtype=float32), 'eval/episode_reward/upright_std': Array(234.91, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8020389080047607, 'eval/sps': 33666.146795739136}
[2025-07-07 15:52:37,965][absl][INFO] - step 4446208
[2025-07-07 15:53:30,303][absl][INFO] - {'eval/walltime': 67.08635640144348, 'training/sps': 11435.973520976473, 'training/walltime': 323.902902841568, 'training/actor_loss': Array(-25.038, dtype=float32), 'training/alpha': Array(0.002, dtype=float32), 'training/alpha_loss': Array(-0., dtype=float32), 'training/buffer_current_size': Array(4.194e+06, dtype=float32), 'training/critic_loss': Array(0.023, dtype=float32), 'eval/episode_reward': Array(279.79, dtype=float32), 'eval/episode_reward/in_target': Array(230.767, dtype=float32), 'eval/episode_reward/upright': Array(622.946, dtype=float32), 'eval/episode_reward_std': Array(321.448, dtype=float32), 'eval/episode_reward/in_target_std': Array(354.074, dtype=float32), 'eval/episode_reward/upright_std': Array(225.313, dtype=float32), 'eval/avg_episode_length': Array(1000., dtype=float32), 'eval/std_episode_length': Array(0., dtype=float32), 'eval/epoch_eval_time': 3.8226919174194336, 'eval/sps': 33484.25736762181}
[2025-07-07 15:53:31,461][absl][INFO] - total steps: 5000960
